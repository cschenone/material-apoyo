---
title: "Estado del Arte de la Investigación sobre técnicas de aprendizaje profundo aplicado al monitoreo, detección y alerta temprana de floraciones de Cianobacterias y Cianotoxinas"
author: "Carlos Schenone"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 5  # up to five depths of headings (specified by #, ## and ...)
    number_sections: true  # if you want number sections at each table header
    theme: united  # specifies the theme style
    highlight: tango  # specifies the syntax highlighting style  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El presente informe describe una metodología para realizar el análisis del estado del arte de las investigaciones basadas en aprendizaje profundo para el monitoreo, detección y alerta temprana de floraciones de cianobacterias y cianotoxinas utilizando R cono lenguaje de desarrollo y la base de datos académica OpenAlex.

## Sobre el tema de interés :: Cianobacterias y Aprendizaje Profundo

Las floraciones de cianobacterias (FC) son eventos de crecimiento acelerado de cianobacterias en el agua, que pueden producir cianotoxinas, compuestos tóxicos para los seres humanos, animales y plantas. El monitoreo, detección y alerta temprana de FC son esenciales para la protección de la salud pública y el medio ambiente.

En los últimos años, el aprendizaje profundo (DL) ha sido aplicado con éxito al monitoreo, detección y alerta temprana de FC. DL es una rama de la inteligencia artificial (IA) que utiliza redes neuronales artificiales para aprender de datos sin supervisión.

### Aproximación al Estado del arte

[Deep learning–based remote sensing estimation of water transparency in shallow lakes by combining Landsat 8 and Sentinel 2 images (2020)](https://sci-hub.se/10.1007/s11356-021-16004-9)
[Research on the Development and Application of a Deep Learning Model for Effective Management and Response to Harmful Algal Blooms (2023)](https://www.mdpi.com/2073-4441/15/12/2293)
[Annual dynamic remote sensing monitoring of phycocyanin concentration in Lake Chaohu based on Sentinel-3 OLCI images](http://www.jlakes.org/ch/reader/view_abstract.aspx?file_no=20220203)
[Spatio-Temporal Variations and Driving Forces of Harmful Algal Blooms in Chaohu Lake: A Multi-Source Remote Sensing Approach (2019)](https://www.mdpi.com/2072-4292/13/3/427/htm)
Análisis de las características de variación espaciotemporal y factores impulsores de la proliferación de cianobacterias en el lago Chaohu basado en imágenes Landsat. Recursos y medio ambiente de la cuenca del río Yangtze.
[Quantitative estimation of phycocyanin concentration using MODIS imagery during the period of cyanobacteria blooming in Taihu Lake (2009)].(https://www.researchgate.net/publication/285526809_Quantitative_estimation_of_phycocyanin_concentration_using_MODIS_imagery_during_the_period_of_cyanobacteria_blooming_in_Taihu_Lake)
Estimación del contenido de ficocianina en el lago Taihu durante el período de brote de cianobacterias basado en imágenes MODIS.


# Metodología para realizar Estado del arte

Una de las técnicas más utilizadas para el análisis del estado del arte es la revisión sistemática en general y el análisis bibliométrico en particular.

En este informe, se presentan las herramientas utilizadas para realizar el análisis del estado del arte de las investigaciones basadas en DL para el monitoreo, detección y alerta temprana de FC. Estas herramientas incluyen:

- R: Un lenguaje de programación para la estadística y el análisis de datos.
- RStudio: Un entorno de desarrollo integrado (IDE) para R
- Github: Un servicio de alojamiento de código fuente en la nube.
- R Markdown: Un formato de documento que permite combinar texto, código y resultados de análisis en un solo documento.
- OpenAlex: Una base de datos bibliográfica abierta de artículos científicos, que incluye millones de entidades interconectadas del sistema de investigación global, ofreciendo información sobre los autores, las instituciones, las revistas y los campos de investigación.
[Acceso al sitio oficial de OpenAlex](https://openalex.org/) 

- OpenAlexR: Una biblioteca de R para acceder a OpenAlex y obtener registros bibliográficos.
[Acceso al Universo OpenAlex](https://ropensci.r-universe.dev/openalexR)
- Bibliometrix: Un paquete R para realizar minería de datos bibliográficos.
[Aria, M. & Cuccurullo, C. (2017) bibliometrix: An R-tool for comprehensive science mapping analysis, Journal of Informetrics, 11(4), pp 959-975, Elsevier](https://www.sciencedirect.com/science/article/abs/pii/S1751157717300500)

Nota: Las bases de datos bibliográficas son herramientas esenciales para los investigadores, ya que brindan acceso a millones de publicaciones científicas, patentes y otros trabajos académicos. Existen varias bases de datos bibliográficas principales que se utilizan ampliamente en diversos campos de investigación. Cada una de estas bases de datos tiene sus propias características y diferencias, y es importante que los investigadores las comprendan para seleccionar la mejor base de datos para sus necesidades.

# Entorno de desarrollo

## Paquetes necesarios para montar el entorno

*Para montar las herramientas necesarias para realizar el análisis del estado del arte, se requiere lo siguiente:*

- Entorno R instalado. [Sitio oficial de R](https://cran.r-project.org/)
- Una cuenta en el repositorio GitHub. [Sitio oficial de GitHub](https://github.com/)
- RStudio instalado. [Sitio oficial de RStudio](https://posit.co/download/rstudio-desktop/)
- La biblioteca de acceso a la API de OpenAlex **openalexR** instalada. [Sitio oficial github de openalexR](https://github.com/ropensci/openalexR)
- La biblioteca de procesamiento de datos bibliométricos **Bibliometrix** instalada. [Sitio oficial de Bibliometrix](https://www.bibliometrix.org/home/)

## Antes de arrancar : pasos previos necesarios para crear un proyecto en RStudio y concetarlo con un repositorio en GitHub

*Para crear un proyecto en RStudio  yvincularlo a un repositorio GitHub, siga estos pasos:*
[Control de versiones con Git y SVN](https://support.posit.co/hc/en-us/articles/200532077)

- Primero deberá activar el sistema Git en RStudio. Para ello, ir a Opciones globales (del menú Herramientas). Hacer clic en Git / SVN. Hacer clic en "Habilite la interfaz de control de versiones para proyectos RStudio". Si es necesario, ingrese la ruta para su ejecutable Git o SVN donde se proporcione. También puede crear o agregar su clave RSA para SSH si es necesario. 
- Cree un nuevo proyecto en RStudio desde Git. Para ello, abra RStudio y haga clic en Crear un nuevo proyecto desde Control de versiones.
- Elija Git y proporcione la URL del repositorio (junto al resto de opciones apropiadas) y luego haga clic en Crear proyecto
- Una vez que haya vinculado su proyecto a GitHub, podrá realizar cambios en su proyecto y subirlos a GitHub.

*Para realizar cambios en el proyecto y actualizar el repositorio GitHub, siga estos pasos:*

- Haga los cambios necesarios en su proyecto.
- Guarde los cambios.
- Realice un commit de los cambios. Para ello, haga clic en Herramientas > Control de versiones > Commit.
- Suba los cambios a GitHub. Para ello, haga clic en Herramientas > Control de versiones > Push branches.

*Para actualizar el proyecto local, siga estos pasos:*
Antes de comenzar a trabajar sobre el proyecto, deberá actualizarlo sincronizando con el repositorio del proyecto en GitHub.

- Descargue los cambios del repositorio al proyecto local. Para ello, haga clic en Herramientas > Control de versiones > Pull Branches.

*Aquí hay un resumen de los pasos para crear un proyecto en RStudio, vincularlo con un repositorio en GitHub y sincronizar los cambios con entre el proyecto local y el repositorio en la nube de GitHub:*

- Cree un nuevo proyecto en RStudio.
- Vincule su proyecto a GitHub.
- Realice cambios en su proyecto.
- Guarde los cambios.
- Realice un commit de los cambios (Commit).
- Suba los cambios a GitHub (Push).
- Antes de comenzar a trabajar sobre un proyecto, descargue los cambios desde Github (Pull)

*Aquí les dejamos un resumen de los pasos para crear un repositorio en Github:*

Para crear un repositorio de Github, debe hacer lo siguiente:

- Ingresar en el sitio web de GitHub: https://github.com/
- Inicie sesión en su cuenta de GitHub. Si aún no tiene una cuenta de GitHub deberá crear una desde el botón Sign up.
- Hacer clic en el botón "Crear un repositorio".
- Ingresar un nombre para el repositorio (por ejemplo el nombre del proyecto) y una descripción.
- Hacer clic en el botón "Crear".

*Aquí les comentamos como crear un archivo R Markdown en RStudio*

Para crear un archivo R Markdown, debe seguir el siguiente procedimiento:

- Abrir RStudio.
- Seleccionar la pestaña "Archivo".
- Hacer clic en el botón "Nuevo documento".
- Seleccionar la opción "Archivo R Markdown".
- Guardar el archivo con la extensión ".Rmd".

# Empezamos a caminar: instalación de los paquetes

Una vez instalado el entorno RStudio y creado un proyecto, empezamos instalando los paquetes que pemiten consultar la API de OpenAlex y realizar el análisis bibliométrico de los datos en el entorno del proyecto.

En caso que el sistema R detecte paquetes con versiones superiores a las instaladas, le consultará si desea actualizarlos a todos, sólo algunos o ninguno. Le recomendamos actualizar todos, **respondiendo "1" a la consulta**.
*"These packages have more recent versions available. It is recommended to update all of them. Which would you like to update?"*

```{r bibliotecas, echo=FALSE}
install.packages("remotes")
remotes::install_github("ropensci/openalexR") ##Developer openalexR version
remotes::install_github("massimoaria/bibliometrix") ##Latest bibliometrix version
```

# Seguimos caminando : carga de las bibliotecas

Ahora, necesitamos cargar en el proyecto las librerías disponibles en los paquetes anteriormente cargados. Además cargamos las librerías *dplyr* y *ggplot* para facilitar el procesamiento de datos y las gráficas.

```{r librerias, echo=FALSE}
options(openalexR.mailto = "example@email.com")
library(openalexR)
library(bibliometrix)
library(dplyr)
library(ggplot2)
```

# Aceleramos la marcha : desarrollo del caso de estudio a partir del análisis del estado del arte de un tema de interés.

A continuación, proponemos el uso de herramientas disponibles en R orientadas a consultar y procesar  información bibliométrica disponible en la base de datos OpenAlex cuyo resultado se orienta a  describir el estado del arte de un tema de interés. Como caso de estudio consideramos las investigaciones desarrolladas en los últimos cinco años relacionadas con las técnicas de aprendizaje profundo aplicadas al monitoreo, detección y alerta temprana de floraciones de cianobaterias obtenidas a partir de los trabajos de investigación indexados en la base de datos OpenAlex.

## Realizar una consulta : Consultar a OpenAlex utilizando openalexR

Objetivo: Descargar todos los **trabajos** que hayan sido **citados más de 50 veces**, **publicados entre 2020 y 2021**, que **incluyan las cadenas “deep learning” o “cyanobacteria” en el título, resumen o contenido** (este útlimo filtro se incluye si está disponible la indexación completa del artículo recuperado), **ordenandos por el total de citas en orden descendente**.

openalexR le ayuda a interactuar con la API OpenAlex para recuperar información bibliográfica sobre publicaciones, autores, lugares, instituciones y conceptos brindando 5 funciones principales:

+ oa_fetch: compone tres funciones a continuación para que el usuario pueda ejecutar todo en un solo paso, es decir, oa_query |> oa_request |> oa2df
+ oa_query: genera una consulta válida, escrita siguiendo la sintaxis de la API OpenAlex, a partir de un conjunto de argumentos proporcionados por el usuario.
+ oa_request: descarga una colección de entidades que coinciden con la consulta creada por oa_query o escrita manualmente por el usuario y devuelve un objeto JSON en formato de lista.
+ oa2df: convierte el objeto JSON en un tibble/marco de datos bibliográfico clásico.
+ oa_random: obtiene una entidad aleatoria, por ejemplo, oa_random("works") proporciona un trabajo diferente cada vez que lo ejecuta

Es importante conocer las posibilidades de los filtros de búsqueda asociados a openalexR. Para acceder a la documentación oficial puede consultar en el [enlace](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/search-entities)

```{r works_search, echo=FALSE}
works_search <- oa_fetch(
  entity = "works",
  search = c("deep learning AND cyanobacteria"),
  cited_by_count = ">50",
  from_publication_date = "2020-01-01",
  to_publication_date = "2021-12-31",
  options = list(sort = "cited_by_count:desc"),
  verbose = TRUE
)
```


### Presentar los resultados en forma de tabla

```{r show_works_search, echo=FALSE}
works_search |>
  show_works() |>
  knitr::kable()
```


## Realizar una consulta : Obtener los n-gramas de las entidades recuperadas (works)

N-grams: OpenAlex ofrece acceso (limitado) a los N-gramas del texto completo de las entidades de tipo works. Dado un vector de IDs de entidades de tipo works, **oa_ngrams** devuelve un dataframe de N-gramas (en la columna de lista de ngrams) para cada trabajo.

A continuación, se consultan los n-gramas de los trabajos recuperados con la consulta.

```{r ngrams_data, echo=FALSE}
#works_identifier <- works_search$id
works_identifier = c("W1963991285", "W1964141474")

ngrams_data <- oa_ngrams(
  works_identifier = works_identifier,
  verbose = TRUE
)
```

### Presentar los resultados en forma de tabla

```{r show_works_ngrams, echo=FALSE}
ngrams_data |>
  knitr::kable()
```

Otro formato de tabla para presentar los resultados.

```{r lapply_ngrams_data, echo=FALSE}
lapply(ngrams_data$ngrams, head, 3) |>
  knitr::kable()
```

### Presentar los resultados en forma de gráficos

```{r ggplot_ngrams_data, echo=FALSE}
ngrams_data |>
  tidyr::unnest(ngrams) |>
  filter(ngram_tokens == 2) |>
  select(id, ngram, ngram_count) |>
  group_by(id) |>
  slice_max(ngram_count, n = 10, with_ties = FALSE) |>
  ggplot(aes(ngram_count, forcats::fct_reorder(ngram, ngram_count))) +
  geom_col(aes(fill = id), show.legend = FALSE) +
  facet_wrap(~id, scales = "free_y") +
  labs(
    title = "Top 10 fulltext bigrams",
    x = "Count",
    y = NULL
)
```

## Procesar los resultados : analizar el resultado utilizando el paquete bibliometrix

Objetivo: analizar el dataframe obtenido de la consulta a OpenAlex utilizando las funciones del paquete bibliometrix

### Conversión de datos

oa2bibliometrix convierte colecciones bibliográficas recopiladas de la (base de datos OpenAlex)[ https://openalex.org/] en un (marco de datos bibliometrix)[https://bibliometrix.org/].
Los nombres de las columnas siguen al formato de [la base de datos Web of Science - WoS](https://images.webofknowledge.com/images/help/WOS/ hs_wos_fieldtags.html).

```{r conversion, echo=FALSE}
df <- oa2df(works_search, entity = "words")
M <- oa2bibliometrix(df)
```

## Análisis bibliométrico

Le recomendamos consultar la documentación del paquete "bibliometrix", donde podrán encontrar información interesante sobre [analisis biliométrico](https://cran.r-project.org/web/packages/bibliometrix/bibliometrix.pdf). 

### Análisis descriptivo

En primer lugar se recomienda realizar un análisis descriptivo del marco de datos bibliográfico.

```{r analisis_descriptivo, echo=FALSE}
results <- biblioAnalysis(M, sep = ";")
```

Luego se propone profundizar el análisis, a partir del uso de un conjunto de funciones provistas por bibliometrix. A continuación se presentan los resultados de aplicar al conjuto de datos las funciones *summary* y *plot*, enfocadas en resumir y graficar los principales indicadores bibliométricos.

La función *summary* muestra en forma de tabla la producción científica anual, artículos ordenados por  número de citas, autores más productivos, países más productivos, citas totales por país y fuentes (revistas) más relevantes. En el caso de las palabras clave de autor, debemos mencionar que OpenAlex no indexa las palabras clave, en su lugar propone el uso de [conceptos](https://docs.openalex.org/api-entities/concepts) por considerarlo un mejor indicador.

```{r summary, echo=FALSE}
options(width=100)
S <- summary(object = results, k = 10, pause = FALSE)
```

A continuación, la función *plot* permite oresentar la información bibliométrica en forma de gráficos.

```{r plot, echo=FALSE}
plot(x = results, k = 10, pause = FALSE)
```

La siguiente operación presenta la variable CR del conjunto de datos M, el cual recordemos contiene el resultado de la consulta a la base de datos OpenAlex en formato bibliométrico.

```{r citations, echo=FALSE}
M$CR[1]
```

La función *citations* permite generar la tabla de frecuencia de las referencias más citadas o de los primeros autores (de referencias) más citados.

Para obtener *los 10 artículos citados* con más frecuencia se puede utilizar la función con el argumento *field = "article"* :

```{r article_citations, echo=FALSE}
CR <- citations(M, field = "article", sep = ";")
cbind(CR$Cited[1:10])
```

Para obtener, *los 10 autores principales más citados* se configura el argumento *field = "author"*

```{r author_citations, echo=FALSE}
CR <- citations(M, field = "author", sep = ";")
cbind(CR$Cited[1:10])
```

La función *localCitations* permite generar la tabla de frecuencia de las referencias locales más citadas.

```{r localcitations, echo=FALSE}
CR <- localCitations(M, sep = ";")
CR$Papers[1:10,]
```

Para obtener *los 10 artículos locales más citados* se opera sobre el conjunto de datos CR obtenido de la función *localCitations*

```{r articulos_localcitations, echo=FALSE}
CR$Papers[1:10,]
```

Para obtener *los 10 autores locales (de referencias) más citados* se opera sobre el conjunto de datos CR obtenido de la función *localCitations*

```{r autores_localcitations, echo=FALSE}
CR$Authors[1:10,]
```

Para obtener *la Clasificación de dominancia de los autores* se dispone de la función *dominance*, la cual calcula la clasificación de dominancia de los autores según lo propuesto por Kumar y Kumar, 2008

```{r autores_dominance, echo=FALSE}
DF <- dominance(results, k = 10)
DF |>
  knitr::kable()
```

El *Índice h de los autores* es una métrica a nivel de autor que intenta medir tanto la productividad como el impacto de las citas de las publicaciones de un científico o académico.

Para calcular el índice h de los primeros 10 autores más productivos se puede operar con el conjunto de datos de la siguiente forma.

```{r autores_indiceh, echo=FALSE}
authors=gsub(","," ",names(results$Authors)[1:10])
indices <- Hindex(M, field = "author", elements=authors, sep = ";", years = 50)
indices$H
```

La *productividad de los principales autores a lo largo del tiempo* se obtiene utilizando la función *authorProdOverTime*, la cual y traza la producción de los autores (en términos de número de publicaciones y citas totales por año) a lo largo del tiempo.

```{r topAU, echo=FALSE}
topAU <- authorProdOverTime(M, k = 10, graph = TRUE)
```

A continuación se presenta la tabla de productividad de los autores por año y la lista de documentos por autor.

```{r topAU_dfAU, echo=FALSE}
head(topAU$dfAU)
```

Relacionado al índice de productividad, se presenta la lista de documentos por autor.

```{r topAU_PapersAU, echo=FALSE}
head(topAU$dfPapersAU)
```

Estimación del *coeficiente de la ley de Lotka*. La función *lotka* estima los coeficientes de la ley de Lotka para la productividad científica (Lotka A.J., 1926).

```{r lotka, echo=FALSE}
L <- lotka(results)
```

Productividad del autor. Distribución empírica.

```{r AuthorProd, echo=FALSE}
L$AuthorProd
```

Estimación del coeficiente beta

```{r beta, echo=FALSE}
L$Beta
```

Constante

```{r constant, echo=FALSE}
L$C
```

Bondad del ajuste

```{r R2, echo=FALSE}
L$R2
```

P-value del test K-S (Kolmogorov-Smirnoff)

```{r p_value, echo=FALSE}
L$p.value
```

La tabla Productividad de autores, obtenida con *L$AuthorProd*, muestra la distribución observada de la productividad científica. El coeficiente Beta estimado anteriormente, junto con la bondad de ajuste y el p resultante de la prueba de dos muestras de Kolmogorov-Smirnoff, tienen como objetivo determinar si existe o no existe una diferencia significativa entre las distribuciones de Lotka observadas y teóricas.

Para acompañar el análisis comparativo de las dos distribuciones, se puede utilizar la siguiente función gráfica.

Distribución observada:

```{r observed, echo=FALSE}
Observed=L$AuthorProd[,3]
```

Distribución teórica con Beta =2:

```{r distribucion_teorica_beta2, echo=FALSE}
Theoretical=10^(log10(L$C)-2*log10(L$AuthorProd[,1]))
```

Gráfico de la productividad científica, donde se presentan la frecuencia de los autores vs articulos para la distribución teórica con beta =2 y la distribución observada.

```{r plot_distribucion_teorica, echo=FALSE}
plot(L$AuthorProd[,1],Theoretical,type="l",col="red",ylim=c(0, 1), xlab="Articles",ylab="Freq. of Authors",main="Scientific Productivity")
lines(L$AuthorProd[,1],Observed,col="blue")
legend(x="topright",c("Theoretical (B=2)","Observed"),col=c("red","blue"),lty = c(1,1,1),cex=0.6,bty="n")
```


## Matrices de redes bibliográficas

Los atributos del artículo están conectados entre sí a través del propio artículo: autor(es) a la revista, palabras clave a la fecha de publicación, etc. Estas conexiones de diferentes atributos generan redes bipartitas que pueden representarse como matrices rectangulares (Artículos x Atributos). Además, las publicaciones científicas contienen referencias a otros trabajos científicos; estas relaciones generan una red adicional, denominada *co-citas* o red de acoplamiento. Estas redes se utilizan para capturar propiedades significativas del sistema de investigación subyacente y, en particular, para determinar la influencia de unidades bibliométricas como académicos y revistas.


### Redes bipartitas

La función *cocMatrix* permite calcular una red bipartita seleccionando uno de los atributos de metadatos.

Por ejemplo, para crear una red de *Artículos x Origen de publicación* debería utilizar la etiqueta de campo "SO".

```{r cocMatrix, echo=FALSE}
A <- cocMatrix(M, Field = "SO", sep = ";")
```

El resultado *A* es una matriz binaria rectangular que representa una red bipartita donde las filas y columnas son artículos y fuentes respectivamente. El elemento genérico *aij* vale *1* si el trabajo  *i* ha sido publicado en la fuente *j*, vale *0* en caso contrario. La suma de la columna *j-ésima* columnas representa el número de manuscritos publicados en la fuente *j*.

Por ejemplo, ordenando en forma decreciente la suma de los valores de las columnas de A se pueden obtener las fuentes de publicación más relevantes.

```{r relevant_publication_sources, echo=FALSE}
sort(Matrix::colSums(A), decreasing = TRUE)[1:10]
```

A continuación se presentan distintas redes bipartitas, obtenidas a partir de la selección de distintos atributos.

*Red de citaciones*

```{r CR_citaciones, echo=FALSE}
A <- cocMatrix(M, Field = "CR", sep = ".  ")
```

*Red de autores*

```{r AU_autores, echo=FALSE}
 A <- cocMatrix(M, Field = "AU", sep = ";")
```

*Red de países*

El atributo *país del autor* no es un atributo estándar del conjunto de datos bibliográficos, por lo cual es necesario extraer esta información del atributo de afiliación utilizando la función *metaTagExtraction*, para luego aplicar la función *cocMatrix* al nuevo atributo calculado.

```{r AU_CO_paises, echo=FALSE}
M <- metaTagExtraction(M, Field = "AU_CO", sep = ";")
A <- cocMatrix(M, Field = "AU_CO", sep = ";")
```

La función *metaTagExtraction* permite extraer las siguientes etiquetas: *países de los autores* (field = "AU_CO"); *países del primer autor* (field = "AU_CO"); *primer autor de cada referencia citada* (field = "CR_AU"); *fuente de publicación de cada referencia citada* (field = "CR_SO") y *afiliaciones de los autores* (field = "AU_UN").

*Red de palabras clave de autor*
En este caso debemos aclarar que la base de datos OpanAlex, no indexa las palabras clave de autor, por lo cual este cálculo no devolverá resultados.

```{r DE_palabras_clave, echo=FALSE}
A <- cocMatrix(M, Field = "DE", sep = ";")
```

*Red de palabras clave PlusOne*

```{r ID_Plusone, echo=FALSE}
A <- cocMatrix(M, Field = "ID", sep = ";")
```


### Acoplamiento bibliográfico

Se dice que dos artículos están bibliográficamente acoplados si al menos una fuente citada aparece en las bibliografías o listas de referencias de ambos artículos (Kessler, 1963).

Se puede obtener una red de acoplamiento utilizando la formulación general: *B = A × AT*, donde *A* es una red bipartita.

El elemento *bij* indica cuántos acoplamientos bibliográficos existen entre los trabajos *i* y *j*. En otras palabras, *bij* indica el número de caminos de longitud 2, a través de los cuales uno se mueve desde *i* a lo largo de la flecha y luego a *j*  la dirección opuesta. 

*B* una matriz simétrica B = BT*

La fuerza del acoplamiento de dos artículos, *i* y *j*  define simplemente por el número de referencias que los artículos tienen en común, dado por el elemento *bij* de la matriz *B*.

La función *biblioNetwork* calcula, a partir de un conjunto de datos bibliográfico, las redes de acoplamiento más utilizadas: Autores, Fuentes y Países. biblioNetwork utiliza dos argumentos para definir la red a calcular:

+ El argumento *analysis* puede ser **co-citation** (co-citación), **coupling** (acoplamiento), **collaboration** (colaboración) o **co-occurrences** (co-ocurrencias).
+ El argumento *network* puede ser **authors** (autores), **references** (referencias), **sources** (fuentes), **countries** (países), **universities** (universidades), **keywords** (palabras clave), **author_keywords** (palabras clave de autor), **titles** (títulos) y **abstracts** (resúmenes).

El siguiente código calcula una red de acoplamiento de artículos clásica:

```{r references_biblioNetwork, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "coupling", network = "references", sep = ".  ")
```

Cabe comentar que si la fuerza del acoplamiento se mide simplemente según el número de referencias que los artículos contienen en común, los artículos con sólo unas pocas referencias tenderían a tener un acoplamiento bibliográfico más débil. Lo cual sugiere que podría ser más práctico cambiar a una medida relativa de acoplamiento bibliográfico. 

```{r authors_biblioNetwork, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "coupling", network = "authors", sep = ";")
```

La función *normalizeSimilarity* calcula la fuerza de asociación, la inclusión, la similitud de Jaccard o Salton entre los vértices de una red. Esta función se puede recuperar directamente desde la función *networkPlot()* usando el argumento normalize.

```{r authors_networkPlot, echo=FALSE}
net=networkPlot(NetMatrix,  normalize = "salton", weighted=NULL, n = 100, Title = "Authors' Coupling", type = "fruchterman",size=5, size.cex=T, remove.multiple=TRUE, labelsize=0.8, label.n=10, label.cex=F)
```

### Cocitación bibliográfica

Hablamos de co-citación de dos artículos cuando ambos son citados en un tercer artículo. Por tanto, la co-citación puede verse como la contraparte del acoplamiento bibliográfico.

Se puede obtener una red de co-citas utilizando la formulación general *C = AT × A*, donde *A* es una red bipartita. Como la matriz *B*, la matriz *C* también es simétrica. La diagonal principal de *C* ntiene el número de casos en los que se cita una referencia en nuestro conjunto de datos. En otras palabras, el elemento diagonal *ci* l número de citas locales de la referencia *i*.

Utilizando la función *biblioNetwork*, se puede calcular una red de cocitación de referencia clásica.

```{r cocitas_biblionetwork, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "references", sep = ". ")
```

### Colaboración bibliográfica

Una red de colaboración científica es una red donde los nodos son autores y los enlaces son coautorías, siendo esta última una de las formas mejor documentadas de colaboración científica (Glanzel, 2004).
Se puede obtener una red de colaboración de autores utilizando la formulación general *AC = AT × A*, donde *A* es una red bipartita de artículos x autores.

El elemento diagonal *aci* es el número de artículos escritos o coescritos por el investigador *i*.

Usando la función *biblioNetwork*, se puede calcular una red de colaboración de autores.

```{r colaboracion_autores_biblionetwork, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "collaboration", network = "authors", sep = ";")
```

La función *biblioNetwork* también permite calcular una red de colaboración de países.

```{r colaboracion_paises_biblionetwork, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "collaboration", network = "countries", sep = ";")
```

## Análisis descriptivo de las características del gráfico de red.

La función *networkStat* calcula varias estadísticas resumidas. En general, partiendo de una matriz bibliográfica (o un objeto igraph), se calculan dos grupos de medidas descriptivas:

+ Las estadísticas resumen de la red.
+ Los índices principales de centralidad y prestigio de los vértices.

A continuación se presenta un ejemplo de una red clásica de co-ocurrencia de palabras clave

```{r co-ocurencia de palabras clave, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences", network = "keywords", sep = ";")
netstat <- networkStat(NetMatrix)
```

Se presenta a continuación el grupo de **estadísticas resumidas** que describen las propiedades estructurales de una red:

+ *Size*: El tamaño es el número de vértices que componen la red;
+ *Density*: La densidad es la proporción de aristas presentes de todas las aristas posibles en la red;
+ *Transitivity*: La transitividad es la relación entre triángulos y ternas conectadas;
+ *Diameter*: El diámetro es la distancia geodésica más larga (longitud del camino más corto entre dos nodos) en la red;
+ *Degree distribution*: La distribución de grados es la distribución acumulativa de los grados de los vértices;
+ *Degree centralization*: La centralización de grados es el grado normalizado de la red general;
+ *Closeness centralization*: La centralización de cercanía es la inversa normalizada de la distancia geodésica promedio del vértice a otros en la red;
+ *Eigenvector centralization*: La centralización de vectores propios es el primer vector propio de la matriz del gráfico;
+ *Betweenness centralization*: La centralización de intermediación es el número normalizado de geodésicas que pasan por el vértice;
+ *Average path length*: La longitud promedio del camino es la media de la distancia más corta entre cada par de vértices de la red.

La siguiente función presenta el nombre dado a cada atributo de la red.

```{r names_network, echo=FALSE}
names(netstat$network)
```


Luego, los principales **índices de centralidad y prestigio de los vértices**, ayudan a identificar los vértices más importantes de una red y la propensión de dos vértices que están conectados a estar ambos conectados a un tercer vértice.

Las estadísticas, a nivel de vértice, devueltas por la función *networkStat* son:

+ *Degree centrality*: Mide la centralidad de grado.
+ *Closeness centrality*: La centralidad de cercanía mide cuántos pasos se requieren para acceder a cada dos vértices desde un vértice determinado;
+ *Eigenvector centrality*: La centralidad del vector propio es una medida de estar bien conectado con los bien conectados;
+ *Betweenness centrality*: La centralidad de intermediación mide el potencial de intermediación o control de acceso. Es (aproximadamente) el número de caminos más cortos entre vértices que pasan por un vértice particular;
+ *PageRank score*: La puntuación PageRank se aproxima a la probabilidad de que cualquier mensaje llegue a un vértice en particular. Este algoritmo fue desarrollado por los fundadores de Google y originalmente se aplicó a enlaces de sitios web;
+ *Hub Score*: estima el valor de los enlaces que salen del vértice. Inicialmente se aplicó a las páginas web;
+ *Authority Score*: La puntuación de autoría es otra medida de centralidad aplicada inicialmente a la Web. Un vértice tiene alta autoridad cuando está vinculado por muchos otros vértices que están vinculando muchos otros vértices;
+ *Vertex Ranking*: La clasificación de vértices es una clasificación general de vértices obtenida como una combinación lineal ponderada de las medidas de vértices de centralidad y prestigio. Los pesos son proporcionales a las cargas del primer componente del Análisis de Componentes Principales.

La siguiente función presenta el nombre dado a cada atributo de los vértices.

```{r names_vertex, echo=FALSE}
names(netstat$vertex)
```

Para resumir los principales resultados de la función *networkStat*, puede utilizar la función genérica *summary*, la cual presenta la información principal sobre la red y la descripción de los vértices a través de varias tablas. La función *summary* acepta un argumento adicional, *k*, para indicar el número de filas de cada tabla. Por ejemplo, al elegir k=10 se limita la presentación de información a los primeros 10 vértices.

```{r netstat, echo=FALSE}
summary(netstat, k=10)
```


## Visualización de redes bibliográficas

Todas las redes bibliográficas se pueden visualizar o modelar gráficamente. En este caso, presentamos la función *networkPlot*, la cual dibuja una red creada por la función *biblioNetwork*. El argumento principal de networkPlot es *type*, indicando el diseño del mapa de la red: circle (círculo), kamada-kawai y mds, entre otros. En caso de elegir type="vosviewer”, la función automáticamente: (i) guarda la red en un archivo de tipo red pajek, llamado “vosnetwork.net”; (ii) inicia una instancia de VOSviewer la cual graficará la red a partir de la información almancenada en el archivo “vosnetwork.net”. En este caso, debemos aclarar que es necesrio declarar la ruta completa de la carpeta donde se encuentra el software VOSviewer utilizando el argumento vos.path,  (por ejemplo, vos.path='c:/software/VOSviewer').

### Colaboración científica de países

A continuación se crea la red de colaboración científica de países.

```{r country_networkcreate, echo=FALSE}
M <- metaTagExtraction(M, Field = "AU_CO", sep = ";")
NetMatrix <- biblioNetwork(M, analysis = "collaboration", network = "countries", sep = ";")
```

Luego, se grafica la red creada anteriormente.

```{r country_networkplot, echo=FALSE}
net=networkPlot(NetMatrix, n = dim(NetMatrix)[1], Title = "Country Collaboration", type = "circle", size=TRUE, remove.multiple=FALSE,labelsize=0.7,cluster="none")
```

### Red de co-citación

A continuación se crea la red de co-citas.

```{r co-citation_networkcreate, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "references", sep = ";")
```

Luego, se grafica la red creada anteriormente.

```{r co-citation_networkplot, echo=FALSE}
net=networkPlot(NetMatrix, n = 30, Title = "Co-Citation Network", type = "fruchterman", size=T, remove.multiple=FALSE, labelsize=0.7,edgesize = 5)
```

### Red de co-ocurrencias de palabras clave

A continuación se crea la red de co-ocurrencias de palabras clave.

Vale aclarar que la base de datos indexada OpenAlex, no indexa las palabras clave de autor. En su lugar calcula la cercanía del artículo con los  [conceptos](https://docs.openalex.org/api-entities/concepts).

```{r keyword_co-occurrences_networkcreate, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences", network = "keywords", sep = ";")
```

Luego, se grafica la red creada anteriormente.

```{r keyword_co-occurrences_networkplot, echo=FALSE}
net=networkPlot(NetMatrix, normalize="association", weighted=T, n = 30, Title = "Keyword Co-occurrences", type = "fruchterman", size=T,edgesize = 5,labelsize=0.7)
```


## Análisis de co-palabras: la estructura conceptual de un campo de investigación

El objetivo del análisis de co-palabras es mapear la estructura conceptual de un conjunto de datos utilizando la co-ocurrencia de palabras en una colección bibliográfica. El análisis se puede realizar mediante técnicas de reducción de dimensionalidad como el *escalado multidimensional* (MDS), el *análisis de correspondencia* (CA) o el *análisis de correspondencia múltiple* (MCA).

Aquí, mostramos un ejemplo utilizando la función **conceptualStructure** que realiza una *CA* o *MCA* para dibujar una estructura conceptual de un área y la técnica de agrupación K-means (K-means clustering) para identificar grupos de documentos que expresan conceptos comunes. Los resultados se trazan en un mapa bidimensional.

La función **conceptualStructure** incluye rutinas de procesamiento del lenguaje natural (NLP) para extraer términos de títulos y resúmenes. Además, implementa el algoritmo de derivación de Porter (porter's stemming) para reducir las palabras a su forma de raíz o base.

Para ampliar la descripción puede consultar la función *termExtraction*.

```{r co-word_analisis, echo=FALSE}
CS <- conceptualStructure(M,field="ID", method="CA", minDegree=4, clust=5, stemming=FALSE, labelsize=10, documents=10)
```


## Red histórica de citas directas

El mapa historiográfico es un gráfico propuesto por E. Garfield (2004) para representar un mapa de red cronológico de las citas directas más relevantes resultantes de una colección bibliográfica. (Garfield, E. (2004). Mapeo historiográfico de la literatura sobre los dominios del conocimiento. Revista de Ciencias de la Información, 30(2), 119-145).

La función *histNetwork* genera una matriz cronológica de red de citas directas.

```{r historical_direct_citation_create, echo=FALSE}
options(width=130)
histResults <- histNetwork(M, min.citations = 1, sep = ";")
```

A continuación, se utiliza la función *histPlot* para graficar el resultado de la matriz de citas directas.

```{r historical_direct_citation_plot, echo=FALSE}
net <- histPlot(histResults, n=15, size = 10, labelsize=5)
```

# Conclusiones

Las técnicas presentadas en este informe permiten realizar un análisis exhaustivo del estado del arte de las investigaciones basadas en DL para el monitoreo, detección y alerta temprana de FC. Estas técnicas pueden ser utilizadas por investigadores, profesionales y otros interesados en este tema para obtener información sobre las tendencias actuales en la investigación, los principales grupos de investigación y las áreas de investigación emergentes.
