---
title: "OpenAlexAPI_a_brief_example_v01"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 5  # up to five depths of headings (specified by #, ## and ...)
    number_sections: true  # if you want number sections at each table header
    theme: united  # specifies the theme style
    highlight: tango  # specifies the syntax highlighting style  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Artificial intelligence (AI) can be understood as “the ability of a machine to perform cognitive functions that we associate with human minds, such as perceiving, reasoning, learning, interacting with the environment, solving problems, and even exercising creativity” (Manyika et al. al., 2017), for which reason different tools to emulate said cognitive functions. Among some of these tools are: neural networks, fuzzy logic, agent-based models, genetic algorithms, cellular automaton, among others.

AI tools have been used in different areas. Machine learning, deep learning, and artificial intelligence have become essential tools for handling and gaining insight from the enormous amounts of data that are being generated via high-performance computing, modern modeling and simulation, and instrument technology.
These methods organize data in a way that is both systematic (collected, processed, and stored methodically according to a standard practice) and semantic (unambiguous and logical, in both order and representation, so that relationships and biases can be easily explored). Once the data is organized, it can then be analyzed using powerful statistical methods. https://www.nas.nasa.gov/hecc/support/kb/machine-learning-overview_572.html

Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it is crucial to support researchers and organisations with novel ways of exploring the scientific landscape as they can take informed decisions. https://arxiv.org/ftp/arxiv/papers/2205/2205.13471.pdf. The results highlight the growing academic interest in research themes like deep learning, machine learning, and internet of things.

Estas herramientas se han usado para dar solución a diferentes clases de problemas, uno de ellos ha sido ayudar al ser humano a mejorar el proceso de toma de decisiones, teniendo en cuenta la racionalidad limitada propia
de las personas, la cual explica que el ser humano toma decisiones de forma parcialmente irracional debido a las limitaciones cognitivas, de información y de tiempo (Simon, H. A. (1979). Rational decision making in business organizations. The American economic review, 69(4), 493-513).

La IA ha ayudado al proceso de toma de decisiones en diferentes áreas del conocimiento. A continuación, se nombran algunas de estas La IA ha ayudado al proceso de toma de decisiones en diferentes áreas del conocimiento. A continuación, se nombran algunas de estas ... 

En las ciencias sociales, Sánchez-Céspedes, D. M., Rodríguez-Miranda, J. P., Salcedo-Parra, O. J. (2020). Análisis de la producción de publicaciones científicas en inteligencia artificial aplicada a la formulación de políticas públicas. Revista Científica, 39(3), 353-368. https://doi.org/10.14483/23448350.16301

En las ciencias de la salud. Sarto, F., Cuccurullo, C., & Aria, M. (2014). Exploring healthcare governance literature: systematic review and paths for future research. Mecosan (https://www.francoangeli.it/Riviste/Scheda_Rivista.aspx?IDarticolo=52780&lingua=en)
D'Aniello, L., Spano, M., Cuccurullo, C., & Aria, M. (2022). Academic Health Centers’ configurations, scientific productivity, and impact: insights from the Italian setting. Health Policy. (https://doi.org/10.1016/j.healthpol.2022.09.007)

En las ciencias económicas. Cuccurullo, C., Aria, M., & Sarto, F. (2013). Twenty years of research on performance management in business and public administration domains. In Academy of Management Proceedings (Vol. 2013, No. 1, p. 14270). Academy of Management (https://doi.org/10.5465/AMBPP.2013.14270abstract)

En las ciencias naturales, la IA ha colaborado en la mejora de los sistemas de alertas tempranas para eventos meteorológicos (Moon et al., 2019; Moreno et al., 2018; Santacreuet al., 2015; Šaur, 2017). Particularmente, el área de la hydrología  presenta dificultades debido al alto grado de complejidad de los procesos. Accelerating Science with AI and Machine Learning. NASA data holdings are growing at a geometric rate, including an estimated 100 petabytes of Earth science imagery alone. Traditional methods of analyzing these data are insufficient to produce answers in a reasonable time frame. Thus, scientists have turned to artificial intelligence and machine learning (AI/ML) methods to facilitate analyzing the massive volumes of data.
https://www.nccs.nasa.gov/news-events/nccs-highlights/acceleratingScience

Related to extracting insight from trained models, it is often said that ML is a black box. While there is—arguably—some sense in which this is true, there is a much more important sense in which we should think about DL models as containing complex, multilayered, structured information that is accessible if we choose to query it.

Recognizing this, our job is one of translation: the information we want is in the models, and we must learn how to translate it into something that is human interpretable. In hydrology, new insights from modeling studies sometimes come from probing models with various types of diagnostic tools (e.g., Martinez & Gupta, 2010; Nearing et al., 2018; Ruddell et al., 2019; Yilmaz et al., 2008), many of which are equally applicable to DL models. Examples of these tools are things like sensitivity analyses to understand (e.g., spatiotemporal) input contributions (e.g., Sundararajan et al., 2017), counterfactuals to understand cause and effect, (e.g., Pearl, 2013; Ribeiro et al., 2016), or DL-specific tools like embedding layers and feature layer analyses (e.g., Bianchi et al., 2020; Wang et al., 2017). It is also at least feasible to leverage advances in explainable AI (XAI; Samek, 2019) to help develop new scientific theory.

Our fear is that if the hydrological sciences community refuses to make a serious investment into the technology that works, then someone else will. This will mean a further decoupling between hydrological science (such as it is) and the societal value that this science is supposed to support. To be clear, the current authors do not want to see that happen, but we are not impressed with the reaction we are seeing in the community. Our message in this opinion piece is to stop assuming that the world needs our theories and expertise and start demonstrating—quantitatively and systematically—the value of individual components of that expertise against the backdrop of a growing importance of big data.

Physics-based models for weather and climate prediction are the standard because these models are trained to incorporate Earth system information and model the natural world as closely as possible. As the length of the observational record increases, it becomes possible to use ML to predict conditions based on past real-world events. For example, Combining Physically Based Modeling and Deep Learning for Fusing GRACE Satellite Data: Can We Learn From Mismatch? Global hydrological models are increasingly being used to assess water availability and sea level rise. Deficiencies in the conceptualization and parameterization in these models may introduce significant uncertainty in model predictions. GRACE satellite senses total water storage at the regional/continental scales. In this study, we applied deep learning to learn the spatial and temporal patterns of mismatch or residual between model simulation and GRACE observations. Key Points: Strong interests exist in fusing GRACE satellite TWS data into global hydrological models to improve their predictive performance, We train CNN deep learning models to learn the mismatch between TWS anomalies simulated by a land surface model and that observed by GRACE, Results show deep learning models significantly improved the predictive skills of land surface model by compensating for missing components.
https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018WR023333

Reconstruction of GRACE Total Water Storage Through Automated Machine Learning. The Gravity Recovery and Climate Experiment (GRACE) satellite mission and its follow-on, GRACE-FO, have provided unprecedented opportunities to quantify the impact of climate extremes and human activities on total water storage at large scales. The ∼1-year data gap between the two GRACE missions needs to be filled to maintain data continuity and maximize mission benefits. In this study, we applied an automated machine learning (AutoML) workflow to perform gridwise GRACE-like data reconstruction. Results further suggest that no single algorithm provides the best predictive performance over the entire CONUS, stressing the importance of using an end-to-end workflow to train, optimize, and combine multiple machine learning models to deliver robust performance, especially when building large-scale hydrological prediction systems and when predictor importance exhibiting strong spatial variability.
https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020WR028666

What Role Does Hydrological Science Play in the Age of Machine Learning? This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall-runoff simulation indicate that there is significantly more information in large-scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence-based preferences for models based on a certain type of “process understanding” that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished. Key Points: Hydrology lacks scale-relevant theories, but deep learning experiments suggest that these theories should exist, The success of machine learning for hydrological forecasting has potential to decouple science from modeling, It is up to hydrologists to clearly show where and when hydrological theory adds value to simulation and forecasting.
https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020WR028091

Thus, this paper aims to provide a bibliometric analysis and review of the current literature on "AI tools to support hidrology" published between 2010 and 2022.

Bibliometrics turns the main tool of science, quantitative analysis, on itself. Essentially, bibliometrics is the application of quantitative analysis and statistics to publications such as journal articles and their accompanying citation counts. Quantitative evaluation of publication and citation data is now used in almost all scientific fields to evaluate growth, maturity, leading authors, conceptual and intellectual maps, trends of a scientific community.

The applied methodology was descriptive with a quantitative approach. Thge following Bibliometric indicators were used to carry out the study:
+ Publication history. This indicator shows the number of publications made year by year.
+ Authors of publications. This shows the list of authors and the number of scientific publications of each one.
+ Author affiliation. This shows the organizations to which the authors belong and the number of publications associated with each organization.
+ Country of origin. This presents the number of publications by country, according to the country where the publication was made.
+ Post type. This indicator shows the number of publications classified by type of media, for example specialized journal, conference report, review, book or book chapter.
+ Sponsor. This indicates the sponsors who funded the research that generated the publications. The number corresponds to the publications associated with each sponsor.
+ Keywords. For the keywords, the strength of association method was used. This method seeks to determine which categories are more related to each other and grouped accordingly. For this, in addition to bibliometrix, the free software VosViewer was used, which is a software for bibliometric analysis that helps to create bibliometric networks to graphically see these relationships.
+ Author citation. This indicator presents how authors are related and cited. The method used to find the relationships and groups is force association, for which the VosViewer software was used, in addition to bibliometrix.

The first step was to develop the search equation and test it in the database selected, which was OpenAlex.

OpenAlex is a fully open catalog of the global research system. It's named after the ancient Library of Alexandria and made by the nonprofit. The OpenAlex dataset describes scholarly entities and how those entities are connected to each other. There are five types of entities: Works are papers, books, datasets, etc; they cite other works, authors are people who create works, venues are journals and repositories that host works, institutions are universities and other orgs that are affiliated with works (via authors), concepts tag Works with a topic.

In order to carry out the data collection we used openalexR (Aria and Cuccurullo, 2022). OpenalexR is free R-package, requieres no authentication and is the primary way to gather bibliographic metadata about publications, authors, venues, institutions, and concepts from OpenAlex. Priem, J., Piwowar, H., & Orr, R. (2022). OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. ArXiv.
Aria, M. and Cuccurullo, C. (2022). openalexR: Gathering Bibliographic Records from ‘OpenAlex’ Database Using ‘DSL’ API. R package version 0.0.1. https://github.com/massimoaria/openalexR.

The query search gooal is to retrieve all papers indexed in OpenAlex database having “artificial intelligence”, “machine learning”, “deep learning”, “data science”, "hydrology", "water" and "vision-based inspection" either in titles or abstracts, published in top journals and conferences during the period 1990 to February 2022 inclusive. https://arxiv.org/abs/2205.01833

Then, we organised all the documents in 7 periods by 5 years based on the publishing year. In each time period, we first identified conceptual themes (i.e., clusters of topics) representing research areas and then we computed the Callon’s indices of density and centrality. These indices allowed us to determine whether the themes are motor, niche, basic, and emerging (or declining). Finally, we mapped the similar themes across the different timeframes and analysed how they developed over time: e.g., before they started being niche and after became motor.
https://arxiv.org/ftp/arxiv/papers/2205/2205.13471.pdf

After obtaining the data collection, the bibliometric indicators were extracted using openalexR and bibliometrix functions; then, the results were analyzed and conclusions were drawn.

Bibliometrix is an open-source tool programmed in R language that provides various routines for performing bibliometric analysis and building data matrices for co-citation, coupling, scientific collaboration analysis and co-word analysis. Aria, M. & Cuccurullo, C. (2017) bibliometrix: An R-tool for comprehensive science mapping analysis, Journal of Informetrics, 11(4), pp 959-975, Elsevier. 
The bibliometrix R-package represents one of the most used science mapping software in the world. Moral-Muñoz et al. (2020) Moral-Muñoz, J. A., Herrera-Viedma, E., Santisteban-Espejo, A., & Cobo, M. J. (2020). Software tools for conducting bibliometric analysis in science: An up-to-date review. Profesional de la Información, 29(1).

Finally, remainder of the paper is organised as follows: in section 1, we prepare the framework to conduct de steps addressed to science mapping. In Section 2, we present our dataset and methodology. In Section 3, we present our results. Finally, Section 4 concludes the paper, outlining future directions.


# Section 1. Prepare framework 

## Install and load the OpenAlex package
You can install and load the released version of openalexR from CRAN with:

```{r Install openalex package, echo=FALSE}
#install.packages("openalexR") ### install openalex package
library(openalexR) ### load openalex package
```

```{r Generate a valid token, echo=FALSE}
#Generate a valid token
#Opeanalex no requiere token de acceso. El paso es necesario en caso que deseemos consultar a una base de datos que requiere suscripción (Scopus, Web of Science, Dimensions)
```

## Install and load the bibliometrix package
You can install and load the released version of bibliometrix from CRAN with:

```{r Install and load the bibliometrix package, echo=FALSE}
#install.packages("bibliometrix") ### install bibliometrix package
library(bibliometrix) ### load bibliometrix package 
```

# Section 2. Data Collection
We initiate the first step in the science mapping analyses obtening data collection throw data loading & converting, then it continues with data analysis and finally data visualization. 

## Data
We make the data behind our projects open. For example, you can download a full dump of the OpenaAlex  database, all 240M+ works entitys, any time. That data is also available via a public, open API with generous rate limits (100,000 calls per day). If you need more, simply drop a line at support@openalex.org. There is a burst rate limit of 10 requests per second. So calling multiple requests at the same time could lead to errors with code 429. More specific details on data dumps and APIs are available on individual project pages.

## Data Loading & Converting
The data source would be a file or online query. In this work, we prefer do online query made through oa_fetch main function of opeanalexR.

oa_fetch() is a composition of functions: oa_query, oa_request and oa2df. As results, oa_query() returns the query string including the OpenAlex endpoint API server address (default). oa_request() downloads the bibliographic records matching the query. Finally, oa2df() converts the final result list to a tibble. 

In your oa_fetch calls, you can specify additional arguments to filter your search result. Available arguments/filters for each entity and associated example values are in the tables below.
https://massimoaria.github.io/openalexR/articles/Filters.html#works

### Define query search 
La construcción de la ecuación de búsqueda será un proceso iterativo, partiendo de tópicos generales, se definen diferentes combinaciones de palabras en la función "oa_fetch". Luego, se analizan los resultados obtenidos buscando identificar palabras o frases no contempladas, que permitan ampliar la búsqueda de  artículos relacionados con el tema y, por otro lado, se analizan los resultados intentando identificar artículos recuperados que no aplican a la temática, mirando en detalle si es posible identificar palabras clave que permitirían excluirlos del conjunto de datos.

El primer grupo de palabras se enfoca en recuperar documentos que aborden la disciplina u objeto de estudio, surgiendo las palabras "hidrology", "hidrological science" y "water". Para definir el listado final, validamos contra la base de conceptos de OpenAlex (https://docs.openalex.org/api-entities/concepts) y los topicos de la Computer Science Ontology - CSO (https://cso.kmi.open.ac.uk/home).

El segundo grupo de palabras se dirige a ubicar aquellos documentos que traten temas relacionados con las técnicas o campos de estudio utilizados para abordar los problemas de la disciplina de interés, en este caso, se seleciconamos las frases "artifical intelligence", "machine learning", "deep learning". Al igual que en la definición del primer grupo, en este caso, validamos contra la base de conceptos de OpenAlex y los tópicos de la Computer Science Ontology (CSO).

El cuarto y quinto grupo de palabras surgen de la traducción de las palabras identificadas del inglés al español, con el objetuivo de capturar aquellos trabajos publicados en revistas de iberoamérica que no estén indexadas en Openalex.

Al final del proceso se aplican las palabras seleccionadas a los parámetros que permiten buscar en el título, abstract y en el cuerpo del trabajo (full text), resultando los siguientes parámetros:

title.search = c((("artificial intelligence","machine learning", "deep learning") AND ("hidrology", "hidrological science", "water")) OR (("inteligencia artificial","aprendizaje automático", "aprendizaje profundo") AND ("hidrología", "ciencia hidrológica", "agua")))

abstract.search = c((("artificial intelligence","machine learning", "deep learning") AND ("hidrology", "hidrological science", "water")) OR (("inteligencia artificial","aprendizaje automático", "aprendizaje profundo") AND ("hidrología", "ciencia hidrológica", "agua")))

fulltext.search = c((("artificial intelligence","machine learning", "deep learning") AND ("hidrology", "hidrological science", "water")) OR (("inteligencia artificial","aprendizaje automático", "aprendizaje profundo") AND ("hidrología", "ciencia hidrológica", "agua")))

### Define publication age
En nuestro caso buscamos analizar el estado del arte en el área de interés, por lo cual decidimos considerar los trabajos publicados en los últimos 5 años, resultando en los siguientes parámetros:

from_publication_date = "2018-01-01"
to_publication_date = "2022-12-31"

### Define relevants works
Consideramos conveniente aplicar un criterio de selección que permita identificar los trabajos más relevantes, para lo cual decidimos utilizar la cantidad de citas como filtro. La identificación del valor mínimo de citas, se logró a través de un proceso iterativo, partiendo de un valor de 100, observando los parámetros bibliométricos de los artículos recuperados, realizando varias iteraciones con distintos valores.

Al final del proceso se identificó el valor de 50 citas como el valor mínimo aceptable, resultando en el siguiente parámetro:

cited_by_count = ">50"

### Prepare the function arguments

```{r Prepare the function arguments: query arguments, include = FALSE}
## oa_query() arguments
entity = "works" #valids args are: works, authors, venues, institutions and concepts
sort = "cited_by_count:desc"
endpoint = "https://api.openalex.org/"
search = NULL
verbose = TRUE
```

```{r Prepare the function arguments: filter arguments, include = FALSE}
## oa_query() filter arguments
title.search = c("artificial intelligence","machine learning", "deep learning", "hidrology", "hidrological science", "water", "inteligencia artificial","aprendizaje automático", "aprendizaje profundo", "hidrología", "ciencia hidrológica", "agua")
abstract.search = c("artificial intelligence","machine learning", "deep learning", "hidrology", "hidrological science", "water", "inteligencia artificial","aprendizaje automático", "aprendizaje profundo", "hidrología", "ciencia hidrológica", "agua")
fulltext.search = c("artificial intelligence","machine learning", "deep learning", "hidrology", "hidrological science", "water", "inteligencia artificial","aprendizaje automático", "aprendizaje profundo", "hidrología", "ciencia hidrológica", "agua")
cited_by_count = ">50"
from_publication_date = "2000-01-01"
to_publication_date = "2022-12-31"
```

```{r oa_request() argument, include = FALSE}
## oa_request() argument
per_page = 200
count_only = FALSE
mailto = "example@email.com"
```

```{r oa2f() arguments, include = FALSE}
# oa2f() arguments
abstract = TRUE
count_only = FALSE
group_by = NULL
```

Then, we need passing all these arguments to the function oa_fetch(), its a composition function to perform query building, requesting, and convert the result to a tibble/data frame or we can use isolate functions oa_query, oa_request and oa2df. In this case we take the last option.

### Perform query building
```{r Perform query building, echo=FALSE}
query <- oa_query(
  entity = entity,
  title.search = title.search,
  cited_by_count = cited_by_count,
  from_publication_date = from_publication_date,
  to_publication_date = to_publication_date,
  search = search,
  sort = sort,
  endpoint = endpoint,
  verbose = verbose
)
```

### Requesting the data
```{r Requesting the data, echo=FALSE}
res <- oa_request(
  query_url = query,
  per_page = per_page,
  count_only = count_only,
  mailto = mailto,
  verbose = verbose
)
```

### Convert the result to a tibble/data frame
```{r Convert the result to a tibble/data frame, echo=FALSE}
df <- oa2df(
  res, 
  entity = entity           
)
```

# Step 3. Data Analysis

## Pre-Exploratory Data Analysis: Calling the API in your browser
Because the API is all GET requests without fancy authentication, you can view any request in your browser. This is a very useful and pleasant way to explore the API and debug scripts; we use it all the time. 
However, this is much nicer if you install an extension to pretty-print the JSON; JSONVue (Chrome) and JSONView (Firefox) are popular, free choices. Here's what an API response looks like with one of these extensions enabled

## EDA (Exploratory Data Analysis) : Exploratory Analyses

### Check number of items retived by our query
Number of items retrieved by our query

```{r Check number of items retived by our query: option 1, echo=FALSE}
#res$count # number of items retrieved by our query
```

```{r Check number of items retived by our query: option 2, echo=FALSE}
dim(df)[1] #number of items retrieved by our query
```

Tibble Dimmensions

```{r results tibble dimmensions, echo=FALSE}
dim(df)
```

### Display final result in a readable form

#### Install and load dplyr and knitr packages

The dplyr library allows you to manipulate and perform calculations on a data frame through a single expression: we are chaining functions with the %>% operator. This syntax or way of coding is called pipe.

```{r load the dplyr library, echo=FALSE}
library(dplyr) # Manipulación de data frames
```

We will present the results in an attractive table, using the kable() function of the knitr library.

```{r load the knitr library, echo=FALSE}
library(knitr) # Presentación de tablas
```

The final result is a complicated tibble, but we can use show_works() to display a simplified version.

```{r Display final result in a readable form: head}
df %>%
  show_works() %>%
  head(5)
```

```{r Display final result in a readable form: knitr::kable, echo=FALSE}
df %>% 
  show_works() %>%
  knitr::kable()
```

```{r Display final result in a readable form: show works, echo=FALSE}
df %>% 
  show_works() %>%
```

Shows results in vertical form

```{r Shows results in vertical form}
df %>%
  show_works() %>%
  glimpse()
```

## using bibliometrix
The first step is to perform a descriptive analysis of the bibliographic data frame. The function biblioAnalysis calculates main bibliometric measures.

### Convert the data frame into a readable and usable format to bibliometrix 

The function oa2bibliometrix converts a bibliographic data frame of works into a bibliometrix object. This object can be used as input collection of a science mapping workflow.

```{r Convert the data frame into a readable and usable format to bibliometrix}
M <- oa2bibliometrix(df)
```

### Descriptive analysis
Main information about the collection: Then, we use the biblioAnalysis and summary functions to perform a descriptive analysis of the data frame.

The function biblioAnalysis returns an object of class “bibliometrix”.

An object of class “bibliometrix” is a list containing the following components:
Articles: the total number of manuscripts
Authors: the authors’ frequency distribution
AuthorsFrac: the authors’ frequency distribution (fractionalized)
FirstAuthors: corresponding author of each manuscript
nAUperPaper: the number of authors per manuscript
Appearances: the number of author appearances
nAuthors: the number of authors
AuMultiAuthoredArt: the number of authors of multi-authored articles
MostCitedPapers: the list of manuscripts sorted by citations
Years: publication year of each manuscript
FirstAffiliation: the affiliation of the corresponding author
Affiliations: the frequency distribution of affiliations (of all co-authors for each paper)
Aff_frac: the fractionalized frequency distribution of affiliations (of all co-authors for each paper)
CO: the affiliation country of the corresponding author
Countries: the affiliation countries’ frequency distribution
CountryCollaboration: the intra-country (SCP) and inter-country (MCP) collaboration indices
TotalCitation: the number of times each manuscript has been cited
TCperYear: the yearly average number of times each manuscript has been cited
Sources: the frequency distribution of sources (journals, books, etc.)
DE: the frequency distribution of authors’ keywords
ID: the frequency distribution of keywords associated to the manuscript by SCOPUS and Thomson Reuters’ ISI Web of Knowledge databases

```{r Descriptive analysis by biblioAnalysis, echo=FALSE}
results <- biblioAnalysis(M, sep = ";")
```

### Summay bibliometric Data
To summarize main results of the bibliometric analysis, use the generic function summary. It displays main information about the bibliographic data frame and several tables, such as annual scientific production, top manuscripts per number of citations, most productive authors, most productive countries, total citation per country, most relevant sources (journals) and most relevant keywords.

Main information table describes the collection size in terms of number of documents, number of authors, number of sources, number of keywords, timespan, and average number of citations.

Furthermore, many different co-authorship indices are shown. In particular, the Authors per Article index is calculated as the ratio between the total number of authors and the total number of articles. The Co-Authors per Articles index is calculated as the average number of co-authors per article. In this case, the index takes into account the author appearances while for the “authors per article” an author, even if he has published more than one article, is counted only once. For that reasons, Authors per Article index ≤
 Co-authors per Article index.

The Collaboration Index (CI) is calculated as Total Authors of Multi-Authored Articles/Total Multi-Authored Articles (Elango and Rajendran, 2012; Koseoglu, 2016). In other word, the Collaboration Index is a Co-authors per Article index calculated only using the multi-authored article set. Elango, B., & Rajendran, P. (2012). Authorship trends and collaboration pattern in the marine sciences literature: a scientometric study. International Journal of Information Dissemination and Technology, 2(3), 166. Koseoglu, M. A. (2016). Mapping the institutional collaboration network of strategic management research: 1980–2014. Scientometrics, 109(1), 203-226.

Summary accepts two additional arguments. k is a formatting value that indicates the number of rows of each table. pause is a logical value (TRUE or FALSE) used to allow (or not) pause in screen scrolling. Choosing k=10 you decide to see the first 10 Authors, the first 10 sources, etc.

```{r Summay bibliometric Data, echo=FALSE}
options(width=100)
summary(results, k = 10, pause = FALSE)
```


### Plot bibliometric Data
Some basic plots can be drawn using the generic function : most productive authors, most productive countries, annual scientist production, average article citations per year, average total citations per year, 

```{r Plot bibliometric Data, echo=FALSE}
plot(x = results, k = 10, pause = FALSE)
```


### Analysis of Cited References
The function citations generates the frequency table of the most cited references or the most cited first authors (of references).
For each manuscript, cited references are in a single string stored in the column “CR” of the data frame.
For a correct extraction, you need to identify the separator field among different references, used by ISI or SCOPUS database. Usually, the default separator is “;” or ".  " (a dot with double space).

#### Reference string of the first manuscript

The figure shows the reference string of the first manuscript. In this case, the separator field is sep = ";".

```{r Reference string of the first manuscrip, echo=FALSE}
M$CR[1]
```

#### Most frequent cited manuscripts

```{r Most frequent cited manuscripts, echo=FALSE}
CR <- citations(M, field = "article", sep = ";")
cbind(CR$Cited[1:10])
```

#### Most frequent cited first authors
```{r Most frequent cited first authors, echo=FALSE}
CR <- citations(M, field = "author", sep = ";")
cbind(CR$Cited[1:10])
```

#### Most frequent local cited authors or documents
Local citations measure how many times an author (or a document) included in this collection have been cited by other authors also in the collection.

```{r Most frequent local cited authors or documents, echo=FALSE}
CR <- localCitations(M, sep = ";")
```

To obtain the most frequent local cited authors:

```{r Most frequent local cited authors, echo=FALSE}
CR$Authors[1:10,]
```

To obtain the most frequent local cited documents:

```{r Most frequent local cited documents, echo=FALSE}
CR$Papers[1:10,]
```


### Authors’ Dominance ranking
The function dominance calculates the authors’ dominance ranking as proposed by Kumar & Kumar, 2008. Kumar, S., & Kumar, S. (2008). Collaboration in research productivity in oil seed research institutes of India. In Proceedings of Fourth International Conference on Webometrics, Informetrics and Scientometrics.

Function arguments are: results (object of class bibliometrix) obtained by biblioAnalysis; and k (the number of authors to consider in the analysis).

The Dominance Factor is a ratio indicating the fraction of multi-authored articles in which a scholar appears as the first author. 

```{r Authors’ Dominance ranking, echo=FALSE}
DF <- dominance(results, k = 10)
DF
CR$Papers[1:10,]
```

Análisis de la figura: "In this example, Kostoff and Holden dominate their research team because they appear as the first authors in all their papers (8 for Kostoff and 3 for Holden)."

### Authors’ h-index
The h-index is an author-level metric that attempts to measure both the productivity and citation impact of the publications of a scientist or scholar. The index is based on the set of the scientist’s most cited papers and the number of citations that they have received in other publications.
The function Hindex calculates the authors’ H-index or the sources’ H-index and its variants (g-index and m-index) in a bibliographic collection.

Function arguments are: M a bibliographic data frame; field is character element that defines the unit of analysis in terms of authors (field = “auhtor”) or sources (field = “source”); elements a character vector containing the authors’ names (or the sources’ names) for which you want to calculate the H-index. The argument has the form c(“SURNAME1 N”,“SURNAME2 N”,…).

In other words, for each author: surname and initials are separated by one blank space. i.e for the authors ARIA MASSIMO and CUCCURULLO CORRADO, elements argument is elements = c(“ARIA M”, “CUCCURULLO C”).

#### The h-index of "Lutz Bornmann" in the collection:
To calculate the h-index of Lutz Bornmann in this collection.

```{r The h-index of "Lutz Bornmann" in the collection, echo=FALSE}
indices <- Hindex(M, field = "author", elements="BORNMANN L", sep = ";", years = 10)

# Bornmann's impact indices:
indices$H
```

#### Citations of "Lutz Bornmann" in the collection
To calculate the citations of Lutz Bornmann in this collection.

```{r Citations of "Lutz Bornmann" in the collection, echo=FALSE}
# Bornmann's citations
indices$CitationList
```

### The h-index of the first 10 most productive authors (in this collection)
To calculate the h-index of the first 10 most productive authors (in this collection)

```{r The h-index of the first 10 most productive authors, echo=FALSE}
authors=gsub(","," ",names(results$Authors)[1:10])
indices <- Hindex(M, field = "author", elements=authors, sep = ";", years = 50)
indices$H
```

### Top-Authors’ Productivity over the Time
The function AuthorProdOverTime calculates and plots the authors’ production (in terms of number of publications, and total citations per year) over the time.

Function arguments are: "res" a bibliographic data frame; k is the number of k Top Authors; graph is a logical. If graph=TRUE, the function plots the author production over time graph.

```{r Top-Authors Productivity over the Time, echo=FALSE}
topAU <- authorProdOverTime(M, k = 10, graph = TRUE)
```

#### Author's productivity per year
To calculate the Author's productivity per year (in this collection)

```{r Authors productivity per year, echo=FALSE}
head(topAU$dfAU)
```

#### Author's documents list
To calculate the Author's document list (in this collection)

```{r Authors documents list, echo=FALSE}
head(topAU$dfPapersAU)
```

### Lotka’s Law coefficient estimation
The function lotka estimates Lotka’s law coefficients for scientific productivity (Lotka A.J., 1926).

Lotka’s law describes the frequency of publication by authors in any given field as an inverse square law, where the number of authors publishing a certain number of articles is a fixed ratio to the number of authors publishing a single article. This assumption implies that the theoretical beta coefficient of Lotka’s law is equal to 2.

Using lotka function is possible to estimate the Beta coefficient of our bibliographic collection and assess, through a statistical test, the similarity of this empirical distribution with the theoretical one.

```{r Lotkas Law coefficient estimation, echo=FALSE}
L <- lotka(results)
```

#### Author Productivity. Empirical Distribution
To calculate the Author Productivity (in this collection). The table "L$AuthorProd" shows the observed distribution of scientific productivity in this collection.

```{r Author Productivity. Empirical Distribution, echo=FALSE}
L$AuthorProd
```

#### Variables estadísticas
To calculate the estimated Beta coefficient, goodness of fit, p-value (Kolmogorov-Smirnoff two sample test).

##### Beta coefficient estimate
```{r Beta coefficient estimate, echo=FALSE}
L$Beta
```

##### Constant
```{r Constant, echo=FALSE}
L$C
```

##### Goodness of fit
```{r Goodness of fit, echo=FALSE}
L$R2
```

##### P-value of K-S two sample test
```{r P-value of K-S two sample test, echo=FALSE}
L$p.value
```

Análisis de los resultados anteriormente obtenidos: "The estimated Beta coefficient is "3.05" with a goodness of fit equal to "0.94". Kolmogorov-Smirnoff two sample test provides a p-value "0.09" that means there is not a significant difference between the observed and the theoretical Lotka distributions"

### Compare the the observed and the theoretical Lotka distributions

#### Observed distribution
```{r Observed distribution, echo=FALSE}
Observed=L$AuthorProd[,3]
```

#### Theoretical distribution with Beta = 2
```{r Theoretical distribution, echo=FALSE}
Theoretical=10^(log10(L$C)-2*log10(L$AuthorProd[,1]))
```

#### Plots observed and the theoretical Lotka distributionss
```{r Plots observed and the theoretical Lotka distributionss, echo=FALSE}
plot(L$AuthorProd[,1],Theoretical,type="l",col="red",ylim=c(0, 1), xlab="Articles",ylab="Freq. of Authors",main="Scientific Productivity")
lines(L$AuthorProd[,1],Observed,col="blue")
legend(x="topright",c("Theoretical (B=2)","Observed"),col=c("red","blue"),lty = c(1,1,1),cex=0.6,bty="n")
```


### Bibliographic network matrices
Manuscript’s attributes are connected to each other through the manuscript itself: author(s) to journal, keywords to publication date, etc. These connections of different attributes generate bipartite networks that can be represented as rectangular matrices (Manuscripts x Attributes).

Furthermore, scientific publications regularly contain references to other scientific works. This generates a further network, namely, co-citation or coupling network.

These networks are analyzed in order to capture meaningful properties of the underlying research system, and in particular to determine the influence of bibliometric units such as scholars and journals.

### Bipartite networks
cocMatrix is a general function to compute a bipartite network selecting one of the metadata attributes.

#### Network Manuscript x Publication Source
To create a network Manuscript x Publication Source you have to use the field tag “SO”

"A" is a rectangular binary matrix, representing a bipartite network where rows and columns are manuscripts and sources respectively.

The generic element "aij" is 1 if the manuscript "i" has been published in source "j", 0 otherwise.

The j−th column sum aj is the number of manuscripts published in source "j"

```{r Network Manuscript x Publication Source, echo=FALSE}
A <- cocMatrix(res, Field = "SO", sep = ";")
```

#### The most relevant publication sources
Sorting, in decreasing order, the column sums of A, you can see the most relevant publication sources

```{r The most relevant publication sources, echo=FALSE}
sort(Matrix::colSums(A), decreasing = TRUE)[1:5]
```

Following this approach, you can compute several bipartite networks.

#### Citation network
```{r Citation network, echo=FALSE}
A <- cocMatrix(res, Field = "CR", sep = ". ")
```

#### Author network
```{r Author network, echo=FALSE}
A <- cocMatrix(res, Field = "AU", sep = ";")
```

#### Country network
Authors’ Countries is not a standard attribute of the bibliographic data frame. You need to extract this information from affiliation attribute using the function metaTagExtraction.

```{r Country network, echo=FALSE}
M <- metaTagExtraction(res, Field = "AU_CO", sep = ";")
# A <- cocMatrix(M, Field = "AU_CO", sep = ";")
```

metaTagExtraction allows to extract the following additional field tags: Authors’ countries (Field = "AU_CO"); First Author’s countries (Field = "AU_CO"); First author of each cited reference (Field = "CR_AU"); Publication source of each cited reference (Field = "CR_SO"); and Authors’ affiliations (Field = "AU_UN").

#### Author keyword network
```{r Author keyword network, echo=FALSE}
A <- cocMatrix(M, Field = "DE", sep = ";")
```

#### Keyword Plus network
```{r Keyword Plus network, echo=FALSE}
A <- cocMatrix(M, Field = "ID", sep = ";")
```

### Bibliographic coupling
Two articles are said to be bibliographically coupled if at least one cited source appears in the bibliographies or reference lists of both articles (Kessler, 1963).

A coupling network can be obtained using the general formulation: B=A×AT, where A is a bipartite network.

Element "bij" indicates how many bibliographic couplings exist between manuscripts "i" and "j"

In other words, "bij" gives the number of paths of length 2, via which one moves from "i" along the arrow and then to "j" in the opposite direction.

B is a symmetrical matrix B=BT

The strength of the coupling of two articles, "i" and "j" is defined simply by the number of references that the articles have in common, as given by the element "bij" of matrix B.

The function biblioNetwork calculates, starting from a bibliographic data frame, the most frequently used coupling networks: Authors, Sources, and Countries.

biblioNetwork uses two arguments to define the network to compute: analysis argument can be “co-citation”, “coupling”, “collaboration”, or “co-occurrences”.

network argument can be “authors”, “references”, “sources”, “countries”, “universities”, “keywords”, “author_keywords”, “titles” and “abstracts”.

#### Article coupling network

The following code calculates a classical article coupling network:

```{r Analysis Article coupling network references, echo=FALSE}
NetMatrix <- biblioNetwork(res, analysis = "coupling", network = "references", sep = ".  ")
```

Articles with only a few references, therefore, would tend to be more weakly bibliographically coupled, if coupling strength is measured simply according to the number of references that articles contain in common.

This suggests that it might be more practical to switch to a relative measure of bibliographic coupling.

normalize Similarity function calculates Association strength, Inclusion, Jaccard or Salton similarity among vertices of a network. normalizeSimilarity can be recalled directly from networkPlot() function using the argument normalize.

```{r Analysis Article coupling network authors, echo=FALSE}
NetMatrix <- biblioNetwork(res, analysis = "coupling", network = "authors", sep = ";")

#net=networkPlot(NetMatrix,  normalize = "salton", weighted=NULL, n = 100, Title = "Authors' Coupling", type = "fruchterman", size=5,size.cex=T,remove.multiple=TRUE,labelsize=0.8,label.n=10,label.cex=F)
```

#### Bibliographic co-citation
We talk about co-citation of two articles when both are cited in a third article. Thus, co-citation can be seen as the counterpart of bibliographic coupling.

A co-citation network can be obtained using the general formulation: C=AT×A, where A is a bipartite network.

Like matrix "B", matrix "C" is also symmetric. The main diagonal of "C" contains the number of cases in which a reference is cited in our data frame.

In other words, the diagonal element "ci" is the number of local citations of the reference "i".

Using the function biblioNetwork, you can calculate a classical reference co-citation network:

```{r Bibliographic co-citation, echo=FALSE}
NetMatrix <- biblioNetwork(res, analysis = "co-citation", network = "references", sep = ".  ")
```

#### Bibliographic collaboration
Scientific collaboration network is a network where nodes are authors and links are co-authorships as the latter is one of the most well-documented forms of scientific collaboration (Glanzel, 2004).

An author collaboration network can be obtained using the general formulation: AC=AT×A, where "A" is a bipartite network Manuscripts x Authors.

The diagonal element "aci" is the number of manuscripts authored or co-authored by researcher "i"

Using the function biblioNetwork, you can calculate an authors’ collaboration network:

```{r Bibliographic collaboration network authors, echo=FALSE}
NetMatrix <- biblioNetwork(res, analysis = "collaboration", network = "authors", sep = ";")
```

or a country collaboration network:

```{r Bibliographic collaboration network countries, echo=FALSE}
NetMatrix <- biblioNetwork(res, analysis = "collaboration", network = "countries", sep = ";")
```

### Descriptive analysis of network graph characteristics
The function networkStat calculates several summary statistics.

In particular, starting from a bibliographic matrix (or an igraph object), two groups of descriptive measures are computed: the summary statistics of the network and the main indices of centrality and prestige of vertices.

#### Keyword co-occurrences network

```{r Keyword co-occurrences network, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences", network = "keywords", sep = ";")
netstat <- networkStat(NetMatrix)
```

#### Summary statistics of the network
This group of statistics allows to describe the structural properties of a network:
Size: is the number of vertices composing the network;
Density: is the proportion of present edges from all possible edges in the network;
Transitivity: is the ratio of triangles to connected triples;
Diameter: is the longest geodesic distance (length of the shortest path between two nodes) in the network;
Degree distribution: is the cumulative distribution of vertex degrees;
Degree centralization: is the normalized degree of the overall network;
Closeness centralization: is the normalized inverse of the vertex average geodesic distance to others in the network;
Eigenvector centralization: is the first eigenvector of the graph matrix;
Betweenness centralization: is the normalized number of geodesics that pass through the vertex;
Average path length: is the mean of the shortest distance between each pair of vertices in the network.

```{r Summary statistics of the networ, echo=FALSE}
names(netstat$network)
```

#### The main indices of centrality and prestige of vertices
These measures help to identify the most important vertices in a network and the propensity of two vertices that are connected to be both connected to a third vertex.

The statistics, at vertex level, returned by networkStat are:
Degree centrality
Closeness centrality: measures how many steps are required to access every other vertex from a given vertex;
Eigenvector centrality: is a measure of being well-connected connected to the well-connected;
Betweenness centrality: measures brokerage or gatekeeping potential. It is (approximately) the number of shortest paths between vertices that pass through a particular vertex;
PageRank score: approximates probability that any message will arrive to a particular vertex. This algorithm was developed by Google founders, and originally applied to website links;
Hub Score: estimates the value of the links outgoing from the vertex. It was initially applied to the web pages;
Authority Score: is another measure of centrality initially applied to the Web. A vertex has high authority when it is linked by many other vertices that are linking many other vertices;
Vertex Ranking: is an overall vertex ranking obtained as a linear weighted combination of the centrality and prestige vertex measures. The weights are proportional to the loadings of the first component of the Principal Component Analysis.

```{r The main indices of centrality and prestige of vertices, echo=FALSE}
names(netstat$vertex)
```

To summarize the main results of the networkStat function, use the generic function summary. It displays the main information about the network and vertex description through several tables.

summary accepts one additional argument. k is a formatting value that indicates the number of rows of each table. Choosing k=10, you decide to see the first 10 vertices.

```{r Summarize the main results of the networkStat function, echo=FALSE}
summary(netstat, k=10)
```

### Visualizing bibliographic networks
All bibliographic networks can be graphically visualized or modeled.

Here, we show how to visualize networks using function networkPlot and VOSviewer software by Nees Jan van Eck and Ludo Waltman (https://www.vosviewer.com).

Using the function networkPlot, you can plot a network created by biblioNetwork using R routines or using VOSviewer.

The main argument of networkPlot is type. It indicates the network map layout: circle, kamada-kawai, mds, etc. Choosing type=“vosviewer”, the function automatically: (i) saves the network into a pajek network file, named “vosnetwork.net”; (ii) starts an instance of VOSviewer which will map the file “vosnetwork.net”. You need to declare, using argument vos.path, the full path of the folder where VOSviewer software is located (es. vos.path=‘c:/software/VOSviewer’).

#### Country Scientific Collaboration

##### Country collaboration network

```{r Country collaboration network, echo=FALSE}
M <- metaTagExtraction(res, Field = "AU_CO", sep = ";")
NetMatrix <- biblioNetwork(M, analysis = "collaboration", network = "countries", sep = ";")
```

##### Plot the network

```{r Plot the country collaboration network, echo=FALSE}
net=networkPlot(NetMatrix, n = dim(NetMatrix)[1], Title = "Country Collaboration", type = "circle", size=TRUE, remove.multiple=FALSE,labelsize=0.7,cluster="none")
```


#### Co-Citation Network

##### Create a co-citation network
```{r Create a co-citation network, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "references", sep = ";")
```

##### Plot the network
```{r Plot the co-citation network, echo=FALSE}
net=networkPlot(NetMatrix, n = 30, Title = "Co-Citation Network", type = "fruchterman", size=T, remove.multiple=FALSE, labelsize=0.7,edgesize = 5)
```


#### Keyword co-occurrences

##### Create keyword co-occurrences network
```{r create keyword co-occurrences network, echo=FALSE}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences", network = "keywords", sep = ";")
```

##### Plot the network
```{r Plot the keyword co-occurrences network, echo=FALSE}
net=networkPlot(NetMatrix, normalize="association", weighted=T, n = 30, Title = "Keyword Co-occurrences", type = "fruchterman", size=T,edgesize = 5,labelsize=0.7)
```


#### Co-Word Analysis: The conceptual structure of a field
The aim of the co-word analysis is to map the conceptual structure of a framework using the word co-occurrences in a bibliographic collection.

The analysis can be performed through dimensionality reduction techniques such as Multidimensional Scaling (MDS), Correspondence Analysis (CA) or Multiple Correspondence Analysis (MCA).

Here, we show an example using the function conceptualStructure that performs a CA or MCA to draw a conceptual structure of the field and K-means clustering to identify clusters of documents which express common concepts. Results are plotted on a two-dimensional map.

conceptualStructure includes natural language processing (NLP) routines (see the function termExtraction) to extract terms from titles and abstracts. In addition, it implements the Porter’s stemming algorithm to reduce inflected (or sometimes derived) words to their word stem, base or root form.

To construct Conceptual Structure Map using keywords, topic dendogram and factorial map of the documentas whit highest contributes use following.   

```{r Co-Word Analysis: The conceptual structure of a field, echo=FALSE}
CS <- conceptualStructure(M,field="ID", method="CA", minDegree=4, clust=5, stemming=FALSE, labelsize=10, documents=10)
```


### Historical Direct Citation Network
The historiographic map is a graph proposed by E. Garfield (2004) to represent a chronological network map of most relevant direct citations resulting from a bibliographic collection.

Garfield, E. (2004). Historiographic mapping of knowledge domains literature. Journal of Information Science, 30(2), 119-145.

The function generates a chronological direct citation network matrix which can be plotted using histPlot.

#### Create a historical citation network

```{r Create a historical citation network, echo=FALSE}
options(width=130)
histResults <- histNetwork(M, min.citations = 1, sep = ";")
```

#### Plot a historical co-citation network
```{r Plot a historical co-citation network, echo=FALSE}
net <- histPlot(histResults, n=15, size = 10, labelsize=5)
```


### EDA (Exploratory Data Analysis)

Track the popularity of Computer science concepts over time, rank institutions by total number of citations and ¿what do the Top 10 institutions publish on?, finally visualize big journals' topics. 

#### Goal: track the popularity of Computer science concepts over time

```{r Goal: track the popularity of Computer science concepts over time}
#Goal: track the popularity of Computer science concepts over time.
#We first download the records of all level-1 concepts/keywords that concern over one million works:
#Concepts: https://github.com/massimoaria/openalexR/commit/2a8da4303447529210ba9b941a5da0cadbe15531

concept_df <- oa_fetch(
  entity = "concepts",
  level = 1,
  ancestors.id = "https://openalex.org/C41008148", #Computer Science
  works_count = ">1000000"
)

concept_df %>%
  select(display_name, counts_by_year) %>%
  tidyr::unnest(counts_by_year) %>%
  filter(year < 2023) %>%
  ggplot() +
  aes(x = year, y = works_count, color = display_name) +
  facet_wrap(~display_name) +
  geom_line(linewidth = 0.7) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    x = NULL, y = "Works count",
    title = "Artificial Intelligence is mayor concept."
  ) +
  guides(color = "none") +
  gghighlight(
    max(works_count) > 244000, 
    label_params = list(nudge_y = 10^5, segment.color = NA)
  )
```

#### Goal: Rank institutions by total number of citations.
We want download all records regarding institutions that are classified as educational (type:education). Again, we check how
many records match the query then download the collection

```{r Goal: Rank institutions by total number of citations}

insts <- oa_fetch(
  entity = "institutions",
##country_code = "cn",
  type = "education",
  verbose = TRUE
)

insts %>%
  slice_max(cited_by_count, n = 10) %>%
  mutate(display_name = forcats::fct_reorder(display_name, cited_by_count)) %>%
  ggplot() +
  aes(x = cited_by_count, y = display_name, fill = display_name) +
  geom_col() +
  scale_fill_viridis_d(option = "E") +
  guides(fill = "none") +
  labs(
    x = "Total citations", y = NULL,
    title = "Top 10 references"
  ) +
  coord_cartesian(expand = FALSE)
```

#### Goal: Top 10 institutions publish on
And, ¿what do the top 10 institutions publish on?

```{r Goal: Top 10 institutions publish on}
concept_cloud <- insts %>%
  select(inst_id = id, x_concepts) %>%
  tidyr::unnest(x_concepts) %>%
  filter(level == 1) %>%
  select(display_name, score) %>%
  group_by(display_name) %>%
  summarise(score = sum(score))

pal <- c("black", scales::brewer_pal(palette = "Set1")(5))
set.seed(1)
wordcloud::wordcloud(
  concept_cloud$display_name,
  concept_cloud$score,
  scale = c(2, .4),
  colors = pal
)
```

#### Goal: Visualize big journals' topics.
We first download all records regarding journals that have published more than 300,000 works, then visualize their scored concepts.

```{r Goal: Visualize big journals topics}
# load concepts data tibble
load(file='~/doctorado/articulo/OpenAlexAPI/data/concept_abbrev.rda')

jours <- oa_fetch(
  entity = "venues",
  works_count = ">500000",
  verbose = TRUE
) %>%
  filter(publisher != "Elsevier"|is.na(publisher)) %>%
  distinct(display_name, .keep_all = TRUE) %>%
  select(jour = display_name, x_concepts) %>%
  tidyr::unnest(x_concepts) %>%
  filter(level == 0) %>%
  left_join(concept_abbrev, by = c("id", "display_name")) %>%
  mutate(abbreviation = gsub(" ", "<br>", abbreviation)) %>%
  tidyr::complete(jour, abbreviation, fill = list(score = 0)) %>%
  group_by(jour) %>%
  mutate(
    color = if_else(score > 10, "#1A1A1A", "#D9D9D9"), # CCCCCC
    label = paste0("<span style='color:", color, "'>", abbreviation, "</span>"))

jours %>%
  ggplot() +
  aes(fill = jour, y = score, x = abbreviation, group = jour) +
  facet_wrap(~jour) +
  geom_hline(yintercept = c(45, 90), colour = "grey90", linewidth = 0.2) +
  geom_segment(
    aes(x = abbreviation, xend = abbreviation, y = 0, yend = 100),
    color = "grey95"
  ) +
  geom_col(color = "grey20") +
  coord_polar(clip = "off") +
  theme_bw() +
  theme(
    plot.background = element_rect(fill = "transparent", colour = NA),
    panel.background = element_rect(fill = "transparent", colour = NA),
    panel.grid = element_blank(),
    panel.border = element_blank(),
    axis.text = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  ggtext::geom_richtext(
    aes(y = 120, label = label),
    fill = NA, label.color = NA, size = 3
  ) +
  scale_fill_brewer(palette = "Set1") +
  guides(fill = "none") +
  labs(y = NULL, x = NULL, title = "Journal clocks")
```


# ANALIZAR
Los siguientes tópicos deberían estar contemplados anteriormente. Sino, analizar si se deben contruir las consultas necesarias para mostrarlos.

## Document x Attribute (Matrix Creation)

## Data Reduction
--- PCA, MDS, MCA, Clustering

## Network Matrix Creation (PCA, MDS, MCA, Clustering)
--- Bibliographic Coupling, Co-citation, Collaboration, Co-occurrence, Historiographic Analysis

# Data Visualization

## Mapping
--- Factorial Map, Dendogram, Semantic Map, Network Map, Historiograph

# Conclusions and next steps

Finally, the challenges, gaps, and future directions are provided to researchers to explore various solutions pertaining to (a) automatic recognition, (b) dataset availability and suitability, (c) efficient data preprocessing techniques, (d) automatic labeling approaches for crack detection, (e) parameter tuning and optimization, (f) using 3D images and data fusion, (g) real-time crack detection, and (h) increasing segmentation accuracy at the pixel level.

# Main Authors’ references (about bibliometrics)
Aria, M. & Cuccurullo, C. (2017). bibliometrix: An R-tool for comprehensive science mapping analysis, Journal of Informetrics, 11(4), pp 959-975, Elsevier, DOI: 10.1016/j.joi.2017.08.007 (https://doi.org/10.1016/j.joi.2017.08.007).

Aria M., Misuraca M., Spano M. (2020) Mapping the evolution of social research and data science on 30 years of Social Indicators Research, Social Indicators Research. (DOI: )https://doi.org/10.1007/s11205-020-02281-3)

Aria, M., Cuccurullo, C., D’Aniello, L., Misuraca, M., & Spano, M. (2022). Thematic Analysis as a New Culturomic Tool: The Social Media Coverage on COVID-19 Pandemic in Italy. Sustainability, 14(6), 3643, (https://doi.org/10.3390/su14063643).

Aria M., Alterisio A., Scandurra A, Pinelli C., D’Aniello B, (2021) The scholar’s best friend: research trends in dog cognitive and behavioural studies, Animal Cognition. (https://doi.org/10.1007/s10071-020-01448-2)

Cuccurullo, C., Aria, M., & Sarto, F. (2016). Foundations and trends in performance management. A twenty-five years bibliometric analysis in business and public administration domains, Scientometrics, DOI: 10.1007/s11192-016-1948-8 (https://doi.org/10.1007/s11192-016-1948-8).

Cuccurullo, C., Aria, M., & Sarto, F. (2015). Twenty years of research on performance management in business and public administration domains. Presentation at the Correspondence Analysis and Related Methods conference (CARME 2015) in September 2015 (https://www.bibliometrix.org/documents/2015Carme_cuccurulloetal.pdf).

Sarto, F., Cuccurullo, C., & Aria, M. (2014). Exploring healthcare governance literature: systematic review and paths for future research. Mecosan (https://www.francoangeli.it/Riviste/Scheda_Rivista.aspx?IDarticolo=52780&lingua=en).

Cuccurullo, C., Aria, M., & Sarto, F. (2013). Twenty years of research on performance management in business and public administration domains. In Academy of Management Proceedings (Vol. 2013, No. 1, p. 14270). Academy of Management (https://doi.org/10.5465/AMBPP.2013.14270abstract).

# Diccionario
SCOPUS (https://www.scopus.com), founded in 2004, offers a great deal of flexibility for the bibliometric user. It permits to query for different fields, such as titles, abstracts, keywords, references and so on. SCOPUS allows for relatively easy downloading data-queries, although there are some limits on very large results sets with over 2,000 items.

Clarivate Analytics Web of Science (WoS) (https://www.webofknowledge.com), owned by Clarivate Analytics, was founded by Eugene Garfield, one of the pioneers of bibliometrics. This platform includes many different collections.

Cochrane Database of Systematic Reviews (https://www.cochranelibrary.com/cdsr/about-cdsr) is the leading resource for systematic reviews in health care. The CDSR includes Cochrane Reviews (the systematic reviews) and protocols for Cochrane Reviews as well as editorials. The CDSR also has occasional supplements. The CDSR is updated regularly as Cochrane Reviews are published “when ready” and form monthly issues; see publication schedule.

PubMed comprises more than 28 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full-text content from PubMed Central and publisher websites.

OpenAlex: An open and comprehensive catalog of scholarly papers, authors, institutions, and more. Inspired by the ancient Library of Alexandria, OpenAlex is an index of hundreds of millions of interconnected entities across the global research system. We're 100% free and open source, and offer access via a web interface, API, and database snapshot. 
Reference: https://openalex.org/

OpenAlex Dataset: The OpenAlex dataset describes scholarly entities and how those entities are connected to each other. There are five types of entities: works, authors, venues, institutions, and concepts.

OpenAlexAPI: access via API to the OpenAlex. Helps you to retrieve bibliographic infomation about publications, authors, venues, institutions and concept from OpenAlex. 
Reference: https://docs.openalex.org/

Bibliometrix: is part of a large family of R packages to perform quantitative research in scientometrics and bibliometrics, called Biblioverse, and developed by the K-Synth team. In addition to bibliometrix, the main packages developed are the following: dimesionsR, pubmedR and openalexR. Bibliometrix support two level of analysis, in one hand a comprehensive set of functions for analytics and graphs for three different level metrics: sources, authors and documents, and the other hand three structures of knowledge (K-structures):  conceptual structure (bibliometric technique: co-word. Statistical techniques: network analysis, factorial analysis, themnatic mapping, thematic evolution, topic modeling), intellectual structure (bibliometric technique: co-citation, citation. Statistical techniques: network analysis, histograph) and social structure (bibliometric technique: collaboration. Statistical techniques: collaboration network).
Reference: https://www.bibliometrix.org/home/index.php/layout/biblioverse
https://www.bibliometrix.org/home/index.php/layout/biblioshiny-2

OpenalexR: There's no officially-supported client library for the API, for now. However, opeanalexR is a third-party libraries built in R. The goal of the openalexR package is to gather bibliographic metadata about publications, authors, venues, institutions, and concepts from OpenAlex using API. (Aria, M. and Cuccurullo, C. (2022). The openalexR offer 5 main functions:
* oa_fetch: composes three functions below so the user can execute everything in one step, i.e., oa_query |> oa_request |> oa2df
* oa_query: generates a valid query, written following the OpenAlex API syntax, from a set of arguments provided by the user.
* oa_request: downloads a collection of entities matching the query created by oa_query or manually written by the user, and returns a JSON object in a list format.
* oa2df: converts the JSON object in classical bibliographic tibble/data frame.
* oa_random: get random entity, e.g., oa_random("works") gives a different work each time you run it
Reference: https://github.com/massimoaria/openalexR
