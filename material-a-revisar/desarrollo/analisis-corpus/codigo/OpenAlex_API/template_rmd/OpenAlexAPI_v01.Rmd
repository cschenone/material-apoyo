---
title: "OpenAlexAPI_v01"
output: html_document
date: "2023-01-18"
# params:
# data: "hawaii"
editor_options: 
  markdown: 
    wrap: 72
descripcion: El documento se propone explorar las posibilidades de consulta a la base de datos indexada OpenAlex, utilizando la API OpenAlex, a través de pruebas de concepto; la primera consulta utilizando palabras o frases como clave de búsqueda, el segundo ejemplo busca un ranking de conceptos dentro de un tema, el tercer caso explora el ranking de instituciones de un país, y el último consulta sobre los tópicos más utilizados en los trabajos con mayor cantidad de citas.   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r R Markdown, include=FALSE}
# R Markdown
# This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# openalexR
# OpenAlexR helps you interface with the OpenAlex API to retrieve bibliographic infomation about publications, authors, venues, institutions and concepts with 5 main functions:
# oa_fetch: composes three functions below so the user can execute everything in one step, i.e., oa_query |> oa_request |> oa2df
# oa_query: generates a valid query, written following the OpenAlex API syntax, from a set of arguments provided by the user.
# oa_request: downloads a collection of entities matching the query created by oa_query or manually written by the user, and returns a JSON object in a list format.
# oa2df: converts the JSON object in classical bibliographic tibble/data frame.
# oa_random: get random entity, e.g., oa_random("works") gives a different work each time you run it
```

# Install packages
```{r Install packages, include = FALSE}
# You can install the developer version of openalexR from GitHub with:
# install.packages("remotes")
# remotes::install_github("massimoaria/openalexR")
# You can install the released version of openalexR from CRAN with:
# install.packages("openalexR", repos = "http://cran.us.r-project.org")

# You can install the released version of rmarkdown from CRAN with:
# install.packages("rmarkdown", repos = "http://cran.us.r-project.org")

# You can install the released version of tidyverse (include dplyr) from CRAN with:
#install.packages("tidyverse", repos = "http://cran.us.r-project.org")

# You can install the released version of wordcloud from CRAN with:
#install.packages("wordcloud", repos = "http://cran.us.r-project.org")

# You can install the released version of ggtext from CRAN with:
#install.packages("ggtext", repos = "http://cran.us.r-project.org")

# You can install the released version of tibble from CRAN with:
#install.packages("tibble", repos = "http://cran.us.r-project.org")

# You can install the released version of ggraph from CRAN with:
#install.packages("ggraph", repos = "http://cran.us.r-project.org")

# You can install the released version of tidygraph from CRAN with:
#install.packages("tidygraph", repos = "http://cran.us.r-project.org")
```

# Optional configurations 
```{r Optional configurations, include = FALSE}
# Before we go any further, we highly recommend you set openalexR.mailto option so that your requests go to the polite pool for faster response times:
# Bonus point if you put this in your .Rprofile with file.edit("~/.Rprofile").
options(openalexR.mailto = "example@email.com")
```

# Load library
```{r load librarys, include = FALSE}
library(openalexR)
library(dplyr)
library(ggplot2)
library(knitr)
library(gghighlight)
library(ggtext)
library(tibble)
library(ggraph)
library(tidygraph)
```

# Goal: Download all works that: have been cited more than 50 times, published between 2000 and 2021, and include the strings "artificial intelligence", "machine learning" or "water resources" in the title. Maybe we also want the results to be sorted by total citations in a descending order.

# To write that query, we should set the function arguments as following

```{r To write that query, we should set the function arguments as following, include = FALSE}
entity = "works"
title.search = c("artificial intelligence", "machine learning", "deep learning", "water resources")
cited_by_count = ">50"
from_publication_date = "2000-01-01"
to_publication_date = "2021-12-31"
sort = "cited_by_count:desc"
verbose = TRUE
```

# Passing all these arguments to the function

```{r Passing all these arguments to the function, echo=FALSE}
test <- oa_fetch(
  entity = entity,
  title.search = title.search,
  cited_by_count = cited_by_count,
  from_publication_date = from_publication_date,
  to_publication_date = to_publication_date,
  sort = sort,
  verbose = TRUE
)
```

Paso 1: Formato de tabla (faltan estilos)

```{r show_works(), echo=FALSE}
test %>%
  show_works() %>%
  knitr::kable()
```

Paso 2: EDA (Exploratory Data Analysis) : Exploratory Analyses

```{r summary() }
test %>%
  show_works() %>%
  summary()
```

```{r head(5)}
test %>%
  show_works() %>%
  head(5)
```

```{r str()}
test %>%
  show_works() %>%
  str()
```

```{r glimpse()}
test %>%
  show_works() %>%
  glimpse()
```

Paso 3: EDA (Exploratory Data Analysis) : Data Analyses

Goal: track the popularity of Computer science concepts over time

```{r Goal: track the popularity of Computer science concepts over time}
#Goal: track the popularity of Computer science concepts over time.
#We first download the records of all level-1 concepts/keywords that concern over one million works:
#Concepts: https://github.com/massimoaria/openalexR/commit/2a8da4303447529210ba9b941a5da0cadbe15531

concept_df <- oa_fetch(
  entity = "concepts",
  level = 1,
  ancestors.id = "https://openalex.org/C41008148", #Computer Science
  works_count = ">1000000"
)

concept_df %>%
  select(display_name, counts_by_year) %>%
  tidyr::unnest(counts_by_year) %>%
  filter(year < 2023) %>%
  ggplot() +
  aes(x = year, y = works_count, color = display_name) +
  facet_wrap(~display_name) +
  geom_line(linewidth = 0.7) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    x = NULL, y = "Works count",
    title = "Artificial Intelligence is mayor concept."
  ) +
  guides(color = "none") +
  gghighlight(
    max(works_count) > 244000, 
    label_params = list(nudge_y = 10^5, segment.color = NA)
  )
```

Goal: Rank institutions in China by total number of citations. We want
download all records regarding chinnese institutions (country_code:cn)
that are classified as educational (type:education). Again, we check how
many records match the query then download the collection

```{r Goal: Rank institutions in China by total number of citations}

china_insts <- oa_fetch(
  entity = "institutions",
  country_code = "cn",
  type = "education",
  verbose = TRUE
)

china_insts %>%
  slice_max(cited_by_count, n = 10) %>%
  mutate(display_name = forcats::fct_reorder(display_name, cited_by_count)) %>%
  ggplot() +
  aes(x = cited_by_count, y = display_name, fill = display_name) +
  geom_col() +
  scale_fill_viridis_d(option = "E") +
  guides(fill = "none") +
  labs(
    x = "Total citations", y = NULL,
    title = "Top 10 Chinnese references"
  ) +
  coord_cartesian(expand = FALSE)
```

Goal: And, ¿ what do the Top 10 Chinnese institutions publish on ?

```{r Goal: And, ¿ what do the Top 10 Chinnese institutions publish on ?}
concept_cloud <- china_insts %>%
  select(inst_id = id, x_concepts) %>%
  tidyr::unnest(x_concepts) %>%
  filter(level == 1) %>%
  select(display_name, score) %>%
  group_by(display_name) %>%
  summarise(score = sum(score))

pal <- c("black", scales::brewer_pal(palette = "Set1")(5))
set.seed(1)
wordcloud::wordcloud(
  concept_cloud$display_name,
  concept_cloud$score,
  scale = c(2, .4),
  colors = pal
)
```

Goal: Visualize big journals' topics. We first download all records regarding journals that have published more than 300,000 works, then visualize their scored concepts.

```{r Goal: Visualize big journals' topics}
load(file='~/doctorado/articulo/OpenAlexAPI/data/concept_abbrev.rda') # Carga tibble de conceptos

jours <- oa_fetch(
  entity = "venues",
  works_count = ">500000",
  verbose = TRUE
) %>%
  filter(publisher != "Elsevier"|is.na(publisher)) %>%
  distinct(display_name, .keep_all = TRUE) %>%
  select(jour = display_name, x_concepts) %>%
  tidyr::unnest(x_concepts) %>%
  filter(level == 0) %>%
  left_join(concept_abbrev, by = c("id", "display_name")) %>%
  mutate(abbreviation = gsub(" ", "<br>", abbreviation)) %>%
  tidyr::complete(jour, abbreviation, fill = list(score = 0)) %>%
  group_by(jour) %>%
  mutate(
    color = if_else(score > 10, "#1A1A1A", "#D9D9D9"), # CCCCCC
    label = paste0("<span style='color:", color, "'>", abbreviation, "</span>"))

jours %>%
  ggplot() +
  aes(fill = jour, y = score, x = abbreviation, group = jour) +
  facet_wrap(~jour) +
  geom_hline(yintercept = c(45, 90), colour = "grey90", linewidth = 0.2) +
  geom_segment(
    aes(x = abbreviation, xend = abbreviation, y = 0, yend = 100),
    color = "grey95"
  ) +
  geom_col(color = "grey20") +
  coord_polar(clip = "off") +
  theme_bw() +
  theme(
    plot.background = element_rect(fill = "transparent", colour = NA),
    panel.background = element_rect(fill = "transparent", colour = NA),
    panel.grid = element_blank(),
    panel.border = element_blank(),
    axis.text = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  ggtext::geom_richtext(
    aes(y = 120, label = label),
    fill = NA, label.color = NA, size = 3
  ) +
  scale_fill_brewer(palette = "Set1") +
  guides(fill = "none") +
  labs(y = NULL, x = NULL, title = "Journal clocks")
```

Goal: Snowball search
The user can also perform snowballing with oa_snowball. Snowballing is a literature search technique where the researcher starts with a set of articles and find articles that cite or were cited by the original set. oa_snowball returns a list of 2 elements: nodes and edges. Similar to oa_fetch, oa_snowball finds and returns information on a core set of articles satisfying certain criteria, but, unlike oa_fetch, it also returns information the articles that cite and are cited by this core set.

```{r Goal: Snowball search}
snowball_docs <- oa_snowball(
  identifier = c("W1964141474", "W1963991285"),
  verbose = TRUE
)

#ggraph(graph = as_tbl_graph(snowball_docs), layout = "stress") +
#  geom_edge_link(aes(alpha = after_stat(index)), show.legend = FALSE) +
#  geom_node_point(aes(fill = oa_input, size = cited_by_count), shape = 21) +
#  geom_node_label(aes(filter = oa_input, label = id), nudge_y = 0.2, size = 3) +
#  scale_edge_width(range = c(0.1, 1.5), guide = "none") +
#  scale_size(range = c(3, 10), guide = "none") +
#  scale_fill_manual(values = c("#1A5878", "#C44237"), na.value = "grey", name = "") +
#  theme_graph() +
#  theme(legend.position = "bottom") +
#  guides(fill = "none")
```
